{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuta\\Anaconda3\\envs\\py27\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the functions\n",
    "\n",
    "def load_file_new(file):\n",
    "    #info of total tweets\n",
    "    tw_info_total = []\n",
    "    for line in open(file) :\n",
    "        tw_info = []\n",
    "        curr_tw = json.loads(line)\n",
    "        #time current tweet is posted\n",
    "        date = curr_tw['citation_date']\n",
    "        #author of this tweet\n",
    "        author = curr_tw['author']['name']\n",
    "        #follower of this user\n",
    "        foll = curr_tw['author']['followers']\n",
    "        #length of tweet\n",
    "        len_tweet = len(curr_tw['tweet']['text'])\n",
    "        #favorite count\n",
    "        favor_ct = curr_tw['tweet']['favorite_count']\n",
    "        #user mentioned count\n",
    "        user_ment_ct = len(curr_tw['tweet']['entities']['user_mentions'])\n",
    "        tw_info.append(date)\n",
    "        tw_info.append(author)\n",
    "        tw_info.append(foll)\n",
    "        tw_info.append(len_tweet)\n",
    "        tw_info.append(favor_ct)\n",
    "        tw_info.append(user_ment_ct)\n",
    "        tw_info_total.append(tw_info)\n",
    "       \n",
    "    df = pd.DataFrame(tw_info_total,columns=['time','author','followers','len of tweet','favorite_count','number of user_mentioned'])\n",
    "    df = df.sort_values(by = 'time')\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_file_test(file):\n",
    "    #info of total tweets\n",
    "    tw_info_total = []\n",
    "    for line in open(file) :\n",
    "        tw_info = []\n",
    "        curr_tw = json.loads(line)\n",
    "        #time current tweet is posted\n",
    "        date = curr_tw['firstpost_date']\n",
    "        #author of this tweet\n",
    "        author = curr_tw['author']['name']\n",
    "        #follower of this user\n",
    "        foll = curr_tw['author']['followers']\n",
    "        #length of tweet\n",
    "        len_tweet = len(curr_tw['tweet']['text'])\n",
    "        #favorite count\n",
    "        favor_ct = curr_tw['tweet']['favorite_count']\n",
    "        #user mentioned count\n",
    "        user_ment_ct = len(curr_tw['tweet']['entities']['user_mentions'])\n",
    "        tw_info.append(date)\n",
    "        tw_info.append(author)\n",
    "        tw_info.append(foll)\n",
    "        tw_info.append(len_tweet)\n",
    "        tw_info.append(favor_ct)\n",
    "        tw_info.append(user_ment_ct)\n",
    "        tw_info_total.append(tw_info)\n",
    "       \n",
    "    df = pd.DataFrame(tw_info_total,columns=['time','author','followers','len of tweet','favorite_count','number of user_mentioned'])\n",
    "    df = df.sort_values(by = 'time')\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_hour(time_stamp):\n",
    "    pst_tz = pytz.timezone('US/Pacific')\n",
    "    return (datetime.fromtimestamp(time_stamp, pst_tz)).hour\n",
    "\n",
    "def avg_rmse_lr(features,labels):\n",
    "    kf = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "    test_mse_list = []\n",
    "\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "        linearregression.fit(X_train, y_train)\n",
    "        \n",
    "        # Compute mse for test set\n",
    "        y_test_pred = linearregression.predict(X_test)\n",
    "        test_mse_list.append(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    avg_test_rmse = np.sqrt(np.mean(test_mse_list))\n",
    "\n",
    "    print ('RMSE for linear regression model is', avg_test_rmse)\n",
    "\n",
    "def avg_rmse_rf(features,labels):\n",
    "    kf = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "    test_mse_list = []\n",
    "\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "        rf_regressor.fit(X_train, y_train)\n",
    "        \n",
    "        # Compute mse for test set\n",
    "        y_test_pred = rf_regressor.predict(X_test)\n",
    "        test_mse_list.append(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    avg_test_rmse = np.sqrt(np.mean(test_mse_list))\n",
    "\n",
    "    print ('RMSE for Random Forest Regressor model is', avg_test_rmse)\n",
    "\n",
    "def avg_rmse_svm(features,labels):\n",
    "    kf = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "    test_mse_list = []\n",
    "\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        # Compute mse for test set\n",
    "        y_test_pred = svm.predict(X_test)\n",
    "        test_mse_list.append(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    avg_test_rmse = np.sqrt(np.mean(test_mse_list))\n",
    "\n",
    "    print ('RMSE for LinearSVR model is', avg_test_rmse)\n",
    "    \n",
    "    \n",
    "def get_features(df):\n",
    "    tw_tot = 0\n",
    "    foll_tot = 0\n",
    "    author_visited_dict = dict()\n",
    "    len_tweet = 0\n",
    "    favorite_ct = 0\n",
    "    user_ment = 0\n",
    "    start_hour = get_hour(df.iloc[0,0])\n",
    "    \n",
    "    feature_list_tot = []\n",
    "    feature_list_curr = []\n",
    "    \n",
    "            \n",
    "    for index, row in df.iterrows():\n",
    "        curr_hour = get_hour(row['time'])\n",
    "        curr_author = row['author']\n",
    "        curr_foll = row['followers'] \n",
    "        curr_len_tweet = row['len of tweet'] \n",
    "        curr_favorite_ct = row['favorite_count']\n",
    "        curr_user_ment = row['number of user_mentioned']\n",
    "        \n",
    "        if curr_hour == start_hour:\n",
    "            tw_tot += 1\n",
    "            len_tweet += curr_len_tweet\n",
    "            favorite_ct += curr_favorite_ct\n",
    "            user_ment += curr_user_ment\n",
    "            if (curr_author not in author_visited_dict):\n",
    "                foll_tot += curr_foll\n",
    "                author_visited_dict[curr_author] = True;\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            feature_list_curr.append(start_hour)\n",
    "            feature_list_curr.append(tw_tot)\n",
    "            feature_list_curr.append(foll_tot)\n",
    "            feature_list_curr.append(len_tweet/tw_tot)\n",
    "            feature_list_curr.append(favorite_ct)\n",
    "            feature_list_curr.append(user_ment)\n",
    "            feature_list_tot.append(feature_list_curr)\n",
    "        \n",
    "            #setup the counters\n",
    "            tw_tot = 1\n",
    "            foll_tot = curr_foll\n",
    "            len_tweet = curr_len_tweet\n",
    "            favorite_ct = curr_favorite_ct\n",
    "            user_ment = curr_user_ment\n",
    "            start_hour = curr_hour\n",
    "            feature_list_curr = []\n",
    "            author_visited_dict.clear()\n",
    "        \n",
    "    feature_df = pd.DataFrame(feature_list_tot,columns=['time','tweets_total','followers_total',\n",
    "                                'len of tweet(avg)','favorite_count','number of user_mentioned'])\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "    \n",
    "def cross_validation_3period_new(df):\n",
    "    # split tweets\n",
    "    time1 = 1422806400 #20150201 8:00am\n",
    "    time2 = 1422849600 #20150201 8:00pm\n",
    "\n",
    "    data_1 = []\n",
    "    data_2 = []\n",
    "    data_3 = []\n",
    "    for i in range(0,len(df)):\n",
    "        tweet = df.iloc[i]\n",
    "        time = tweet[\"time\"]\n",
    "        if   time < time1: \n",
    "             data_1.append(tweet)\n",
    "        elif time >= time1 and time < time2: \n",
    "             data_2.append(tweet)\n",
    "        else: \n",
    "             data_3.append(tweet)\n",
    "\n",
    "    df_1 = pd.DataFrame(data_1,columns=['time','author','followers',\n",
    "                                             'len of tweet','favorite_count','number of user_mentioned'])\n",
    "    df_1 = df_1.sort_values(by = 'time')\n",
    "    df_1 = df_1.reset_index(drop=True)\n",
    "\n",
    "    df_2 = pd.DataFrame(data_2,columns=['time','author','followers',\n",
    "                                             'len of tweet','favorite_count','number of user_mentioned'])\n",
    "    df_2 = df_2.sort_values(by = 'time')\n",
    "    df_2 = df_2.reset_index(drop=True)\n",
    "    \n",
    "    df_3 = pd.DataFrame(data_3,columns=['time','author','followers',\n",
    "                                             'len of tweet','favorite_count','number of user_mentioned'])\n",
    "    df_3 = df_3.sort_values(by = 'time')\n",
    "    df_3 = df_3.reset_index(drop=True)\n",
    "\n",
    "    # first period\n",
    "    feature_df_1 = get_features(df_1)\n",
    "    y_1 = np.nan_to_num(feature_df_1['tweets_total'].values)\n",
    "    feature_df_1.drop(columns =['time'],inplace = True)\n",
    "    x_1 = np.nan_to_num(feature_df_1.values)\n",
    "    x_1_len = x_1.shape[0]\n",
    "    train_1 = x_1[0:x_1_len-1,:]\n",
    "    y_1_len = len(y_1)\n",
    "    target_1 = y_1[1:y_1_len]\n",
    "\n",
    "    # rmse_1\n",
    "    print \"first period\"\n",
    "    avg_rmse_lr(train_1,target_1)\n",
    "    avg_rmse_rf(train_1,target_1)\n",
    "    avg_rmse_svm(train_1,target_1)\n",
    "    \n",
    "    # second period\n",
    "    feature_df_2 = get_features(df_2)\n",
    "    y_2 = np.nan_to_num(feature_df_2['tweets_total'].values)\n",
    "    feature_df_2.drop(columns =['time'],inplace = True)\n",
    "    x_2 = np.nan_to_num(feature_df_2.values) \n",
    "    x_2_len = x_2.shape[0]\n",
    "    train_2 = x_2[0:x_2_len-1,:]\n",
    "    y_2_len = len(y_2)\n",
    "    target_2 = y_2[1:y_2_len]\n",
    "\n",
    "    # rmse_2\n",
    "    print \"second period\"\n",
    "    avg_rmse_lr(train_2,target_2)\n",
    "    avg_rmse_rf(train_2,target_2)\n",
    "    avg_rmse_svm(train_2,target_2)\n",
    "    \n",
    "    \n",
    "    # third period\n",
    "    feature_df_3 = get_features(df_3)\n",
    "    y_3 = np.nan_to_num(feature_df_3['tweets_total'].values)\n",
    "    feature_df_3.drop(columns =['time'],inplace = True)\n",
    "    x_3 = np.nan_to_num(feature_df_3.values)\n",
    "    x_3_len = x_3.shape[0]\n",
    "    train_3 = x_3[0:x_3_len-1,:]\n",
    "    y_3_len = len(y_3)\n",
    "    target_3 = y_3[1:y_3_len]\n",
    "\n",
    "    # rmse_3\n",
    "    print \"third period\"\n",
    "    avg_rmse_lr(train_3,target_3)\n",
    "    avg_rmse_rf(train_3,target_3)\n",
    "    avg_rmse_svm(train_3,target_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files[0] => tweets_#gohawks.txt\n",
      "files[1] => tweets_#gopatriots.txt\n",
      "files[2] => tweets_#nfl.txt\n",
      "files[3] => tweets_#patriots.txt\n",
      "files[4] => tweets_#sb49.txt\n",
      "files[5] => tweets_#superbowl.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"tweet_data/\"\n",
    "\n",
    "files = [\"tweets_#gohawks.txt\", \"tweets_#gopatriots.txt\", \n",
    "        \"tweets_#nfl.txt\", \"tweets_#patriots.txt\", \n",
    "        \"tweets_#sb49.txt\", \"tweets_#superbowl.txt\"]\n",
    "\n",
    "for index, name in enumerate(files):\n",
    "    print (\"files[\" + str(index) + \"] => \" + name)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearregression = LinearRegression()\n",
    "rf_regressor = RandomForestRegressor(n_estimators=13,\n",
    "                             max_features=3,\n",
    "                             max_depth=11,\n",
    "                             bootstrap=True,\n",
    "                             oob_score=True,\n",
    "                             random_state=0)\n",
    "svm = LinearSVR(random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.4 (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag gohawks\n",
      "first period\n",
      "('RMSE for linear regression model is', 1671.571821081425)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuta\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\ensemble\\forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RMSE for Random Forest Regressor model is', 1096.1852083005842)\n",
      "('RMSE for LinearSVR model is', 1100.2600887786016)\n",
      "second period\n",
      "('RMSE for linear regression model is', 20266.900027210446)\n",
      "('RMSE for Random Forest Regressor model is', 2769.1677585780044)\n",
      "('RMSE for LinearSVR model is', 6514.048278880562)\n",
      "third period\n",
      "('RMSE for linear regression model is', 214.7525014340775)\n",
      "('RMSE for Random Forest Regressor model is', 66.92900398846412)\n",
      "('RMSE for LinearSVR model is', 383.0070664688258)\n"
     ]
    }
   ],
   "source": [
    "# hashtag tweets_#gohawks \n",
    "\n",
    "# load #gohawks\n",
    "gohawks = load_file_new('tweet_data/tweets_#gohawks.txt')\n",
    "print 'hashtag gohawks'\n",
    "cross_validation_3period_new(gohawks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag gopatriots \n",
      "first period\n",
      "('RMSE for linear regression model is', 60.64625904551412)\n",
      "('RMSE for Random Forest Regressor model is', 61.618926891704106)\n",
      "('RMSE for LinearSVR model is', 82.30775393437325)\n",
      "second period\n",
      "('RMSE for linear regression model is', 2217.6928222474935)\n",
      "('RMSE for Random Forest Regressor model is', 1128.6984757287598)\n",
      "('RMSE for LinearSVR model is', 1455.8720110671686)\n",
      "third period\n",
      "('RMSE for linear regression model is', 22.895030583627182)\n",
      "('RMSE for Random Forest Regressor model is', 8.919631297604576)\n",
      "('RMSE for LinearSVR model is', 98.13851963200935)\n"
     ]
    }
   ],
   "source": [
    "# hashtag tweets_#gopatriots \n",
    "# load #gopatriots\n",
    "gopatriots  = load_file_new('tweet_data/tweets_#gopatriots.txt')\n",
    "print 'hashtag gopatriots '\n",
    "cross_validation_3period_new(gopatriots) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag nfl\n",
      "first period\n",
      "('RMSE for linear regression model is', 318.1609315476443)\n",
      "('RMSE for Random Forest Regressor model is', 284.64714221884753)\n",
      "('RMSE for LinearSVR model is', 1630.519554138228)\n",
      "second period\n",
      "('RMSE for linear regression model is', 4326.9571395384555)\n",
      "('RMSE for Random Forest Regressor model is', 3158.7069904439204)\n",
      "('RMSE for LinearSVR model is', 7457.270071841277)\n",
      "third period\n",
      "('RMSE for linear regression model is', 156.25726821547462)\n",
      "('RMSE for Random Forest Regressor model is', 166.93016726413404)\n",
      "('RMSE for LinearSVR model is', 1733.4765280817935)\n"
     ]
    }
   ],
   "source": [
    "# hashtag tweets_#nfl\n",
    "# load #nfl\n",
    "nfl = load_file_new('tweet_data/tweets_#nfl.txt')\n",
    "print 'hashtag nfl'\n",
    "cross_validation_3period_new(nfl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag patriots\n",
      "first period\n",
      "('RMSE for linear regression model is', 756.3088025731204)\n",
      "('RMSE for Random Forest Regressor model is', 749.8751142851752)\n",
      "('RMSE for LinearSVR model is', 1437.1633280867065)\n",
      "second period\n",
      "('RMSE for linear regression model is', 26082.288753840166)\n",
      "('RMSE for Random Forest Regressor model is', 18435.08080132765)\n",
      "('RMSE for LinearSVR model is', 21335.436806684756)\n",
      "third period\n",
      "('RMSE for linear regression model is', 319.02870954874817)\n",
      "('RMSE for Random Forest Regressor model is', 153.14366527634579)\n",
      "('RMSE for LinearSVR model is', 3043.9832472180296)\n"
     ]
    }
   ],
   "source": [
    "# hashtag tweets_#patriots \n",
    "# load #patriots\n",
    "patriots = load_file_new('tweet_data/tweets_#patriots.txt')\n",
    "print 'hashtag patriots'\n",
    "cross_validation_3period_new(patriots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag sb49\n",
      "first period\n",
      "('RMSE for linear regression model is', 99.92309893343163)\n",
      "('RMSE for Random Forest Regressor model is', 118.65700915657258)\n",
      "('RMSE for LinearSVR model is', 743.363651130486)\n",
      "second period\n",
      "('RMSE for linear regression model is', 319765.74488015106)\n",
      "('RMSE for Random Forest Regressor model is', 33098.93104544145)\n",
      "('RMSE for LinearSVR model is', 33622.279871172745)\n",
      "third period\n",
      "('RMSE for linear regression model is', 1166.0111578620972)\n",
      "('RMSE for Random Forest Regressor model is', 230.5987330029301)\n",
      "('RMSE for LinearSVR model is', 9429.138141534404)\n"
     ]
    }
   ],
   "source": [
    "# hashtag tweets_#sb49 \n",
    "# load #sb49\n",
    "sb49 = load_file_new('tweet_data/tweets_#sb49.txt')\n",
    "print 'hashtag sb49'\n",
    "cross_validation_3period_new(sb49)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag superbowl\n",
      "first period\n",
      "('RMSE for linear regression model is', 800.3889383965305)\n",
      "('RMSE for Random Forest Regressor model is', 785.0638115117295)\n",
      "('RMSE for LinearSVR model is', 34070.959436975594)\n",
      "second period\n",
      "('RMSE for linear regression model is', 627468.4500797167)\n",
      "('RMSE for Random Forest Regressor model is', 68842.09401419864)\n",
      "('RMSE for LinearSVR model is', 142623.71478833762)\n",
      "third period\n",
      "('RMSE for linear regression model is', 630.0717465460622)\n",
      "('RMSE for Random Forest Regressor model is', 447.0603729858627)\n",
      "('RMSE for LinearSVR model is', 5889.797743610945)\n"
     ]
    }
   ],
   "source": [
    "# hashtag tweets_#superbowl \n",
    "# load #superbowl\n",
    "\n",
    "superbowl = load_file_new('tweet_data/tweets_#superbowl.txt')\n",
    "print 'hashtag superbowl'\n",
    "cross_validation_3period_new(superbowl) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (ii）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag all hashtags\n",
      "first period\n",
      "('RMSE for linear regression model is', 2551.3706912759853)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuta\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\ensemble\\forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RMSE for Random Forest Regressor model is', 2685.010457186324)\n",
      "('RMSE for LinearSVR model is', 18282.941331870403)\n",
      "second period\n",
      "('RMSE for linear regression model is', 540809.654549252)\n",
      "('RMSE for Random Forest Regressor model is', 97771.44100768653)\n",
      "('RMSE for LinearSVR model is', 160041.66435283766)\n",
      "third period\n",
      "('RMSE for linear regression model is', 1589.7368697188265)\n",
      "('RMSE for Random Forest Regressor model is', 691.5662886364768)\n",
      "('RMSE for LinearSVR model is', 12830.72981868729)\n"
     ]
    }
   ],
   "source": [
    "# all hashtags aggregated data\n",
    "\n",
    "tw_info_all = []\n",
    "\n",
    "for hashtag in files:\n",
    "    for line in open(path + hashtag, 'r') :\n",
    "        tw_info = []\n",
    "        curr_tw = json.loads(line)\n",
    "        #time current tweet is posted\n",
    "        date = curr_tw['citation_date']\n",
    "        #author of this tweet\n",
    "        author = curr_tw['author']['name']\n",
    "        #follower of this user\n",
    "        foll = curr_tw['author']['followers']\n",
    "        #length of tweet\n",
    "        len_tweet = len(curr_tw['tweet']['text'])\n",
    "        #favorite count\n",
    "        favor_ct = curr_tw['tweet']['favorite_count']\n",
    "        #user mentioned count\n",
    "        user_ment_ct = len(curr_tw['tweet']['entities']['user_mentions'])\n",
    "        tw_info.append(date)\n",
    "        tw_info.append(author)\n",
    "        tw_info.append(foll)\n",
    "        tw_info.append(len_tweet)\n",
    "        tw_info.append(favor_ct)\n",
    "        tw_info.append(user_ment_ct)\n",
    "        tw_info_all.append(tw_info)\n",
    "    \n",
    "df_all = pd.DataFrame(tw_info_all,columns=['time','author','followers',\n",
    "                                             'len of tweet','favorite_count','number of user_mentioned'])\n",
    "df_all = df_all.sort_values(by = 'time')\n",
    "df_all = df_all.reset_index(drop=True)\n",
    "\n",
    "print 'hashtag all hashtags'\n",
    "cross_validation_3period_new(df_all) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using random forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "period_1_samples = ['test_data/sample1_period1.txt','test_data/sample4_period1.txt','test_data/sample5_period1.txt','test_data/sample8_period1.txt']\n",
    "period_2_samples = ['test_data/sample2_period2.txt','test_data/sample6_period2.txt','test_data/sample9_period2.txt']\n",
    "period_3_samples = ['test_data/sample3_period3.txt','test_data/sample7_period3.txt','test_data/sample10_period3.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "tw_info_all = []\n",
    "\n",
    "for hashtag in files:\n",
    "    for line in open(path + hashtag, 'r') :\n",
    "        tw_info = []\n",
    "        curr_tw = json.loads(line)\n",
    "        #time current tweet is posted\n",
    "        date = curr_tw['firstpost_date']\n",
    "        #author of this tweet\n",
    "        author = curr_tw['author']['name']\n",
    "        #follower of this user\n",
    "        foll = curr_tw['author']['followers']\n",
    "        #length of tweet\n",
    "        len_tweet = len(curr_tw['tweet']['text'])\n",
    "        #favorite count\n",
    "        favor_ct = curr_tw['tweet']['favorite_count']\n",
    "        #user mentioned count\n",
    "        user_ment_ct = len(curr_tw['tweet']['entities']['user_mentions'])\n",
    "        tw_info.append(date)\n",
    "        tw_info.append(author)\n",
    "        tw_info.append(foll)\n",
    "        tw_info.append(len_tweet)\n",
    "        tw_info.append(favor_ct)\n",
    "        tw_info.append(user_ment_ct)\n",
    "        tw_info_all.append(tw_info)\n",
    "    \n",
    "df_all = pd.DataFrame(tw_info_all,columns=['time','author','followers',\n",
    "                                             'len of tweet','favorite_count','number of user_mentioned'])\n",
    "df_all = df_all.sort_values(by = 'time')\n",
    "df_all = df_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuta\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\ensemble\\forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data/sample1_period1.txt\n",
      "[215.03846154 181.32564103 170.63589744 298.58974359]\n",
      "test_data/sample4_period1.txt\n",
      "[526.40769231 384.93205128 244.7481685  237.97530825]\n",
      "test_data/sample5_period1.txt\n",
      "[ 518.54807692 1393.42655678  519.68296703  250.78579882]\n",
      "test_data/sample8_period1.txt\n",
      "[200.05128205 154.71794872 148.91575092]\n",
      "test_data/sample2_period2.txt\n",
      "[133388.53846154 133388.53846154 133388.53846154 135944.        ]\n",
      "test_data/sample6_period2.txt\n",
      "[133388.53846154 135944.         184159.38461538 171287.30769231]\n",
      "test_data/sample9_period2.txt\n",
      "[133388.53846154 133388.53846154 133388.53846154 133388.53846154]\n",
      "test_data/sample3_period3.txt\n",
      "[ 761.46153846  867.80769231 1004.57692308  991.11965812]\n",
      "test_data/sample7_period3.txt\n",
      "[65.69230769 64.53846154 63.         57.23076923]\n",
      "test_data/sample10_period3.txt\n",
      "[57.23076923 57.23076923 57.23076923 59.07692308]\n"
     ]
    }
   ],
   "source": [
    "# split tweets\n",
    "time1 = 1422806400 #20150201 8:00am\n",
    "time2 = 1422849600 #20150201 8:00pm\n",
    "\n",
    "data_1 = []\n",
    "data_2 = []\n",
    "data_3 = []\n",
    "for i in range(0,len(df_all)):\n",
    "    tweet = df_all.iloc[i]\n",
    "    time = tweet[\"time\"]\n",
    "    if   time < time1: \n",
    "         data_1.append(tweet)\n",
    "    elif time >= time1 and time < time2: \n",
    "         data_2.append(tweet)\n",
    "    else: \n",
    "         data_3.append(tweet)\n",
    "\n",
    "df_1 = pd.DataFrame(data_1,columns=['time','author','followers',\n",
    "                                             'len of tweet','favorite_count','number of user_mentioned'])\n",
    "df_1 = df_1.sort_values(by = 'time')\n",
    "df_1 = df_1.reset_index(drop=True)\n",
    "\n",
    "df_2 = pd.DataFrame(data_2,columns=['time','author','followers',\n",
    "                                             'len of tweet','favorite_count','number of user_mentioned'])\n",
    "df_2 = df_2.sort_values(by = 'time')\n",
    "df_2 = df_2.reset_index(drop=True)\n",
    "    \n",
    "df_3 = pd.DataFrame(data_3,columns=['time','author','followers',\n",
    "                                             'len of tweet','favorite_count','number of user_mentioned'])\n",
    "df_3 = df_3.sort_values(by = 'time')\n",
    "df_3 = df_3.reset_index(drop=True)\n",
    "\n",
    "# first period\n",
    "feature_df_1 = get_features(df_1)\n",
    "y_1 = np.nan_to_num(feature_df_1['tweets_total'].values)\n",
    "feature_df_1.drop(columns =['time'],inplace = True)\n",
    "x_1 = np.nan_to_num(feature_df_1.values)\n",
    "x_1_len = x_1.shape[0]\n",
    "train_1 = x_1[0:x_1_len-1,:]\n",
    "y_1_len = len(y_1)\n",
    "target_1 = y_1[1:y_1_len]\n",
    "rf_regressor_1 = rf_regressor.fit(train_1,target_1)\n",
    "\n",
    "for sample in period_1_samples:\n",
    "    df_test = load_file_test(sample)\n",
    "    feature_df_test = get_features(df_test)\n",
    "    feature_df_test.drop(columns =['time'],inplace = True)\n",
    "    x_test = np.nan_to_num(feature_df_test.values)\n",
    "    x_test_len = x_test.shape[0]\n",
    "    test = x_test[0:x_test_len-1,:]\n",
    "    predicted_test = rf_regressor_1.predict(test)\n",
    "    print sample\n",
    "    print predicted_test\n",
    "    \n",
    "# second period\n",
    "feature_df_2 = get_features(df_2)\n",
    "y_2 = np.nan_to_num(feature_df_2['tweets_total'].values)\n",
    "feature_df_2.drop(columns =['time'],inplace = True)\n",
    "x_2 = np.nan_to_num(feature_df_2.values)\n",
    "x_2_len = x_2.shape[0]\n",
    "train_2 = x_2[0:x_2_len-1,:]\n",
    "y_2_len = len(y_2)\n",
    "target_2 = y_2[1:y_2_len]\n",
    "rf_regressor_2 = rf_regressor.fit(train_2,target_2)\n",
    "\n",
    "for sample in period_2_samples:\n",
    "    df_test = load_file_test(sample)\n",
    "    feature_df_test = get_features(df_test)\n",
    "    feature_df_test.drop(columns =['time'],inplace = True)\n",
    "    x_test = np.nan_to_num(feature_df_test.values)\n",
    "    x_test_len = x_test.shape[0]\n",
    "    test = x_test[0:x_test_len-1,:]\n",
    "    predicted_test = rf_regressor_2.predict(test)\n",
    "    print sample\n",
    "    print predicted_test\n",
    "\n",
    "# third period\n",
    "feature_df_3 = get_features(df_3)\n",
    "y_3 = np.nan_to_num(feature_df_3['tweets_total'].values)\n",
    "feature_df_3.drop(columns =['time'],inplace = True)\n",
    "x_3 = np.nan_to_num(feature_df_3.values)\n",
    "x_3_len = x_3.shape[0]\n",
    "train_3 = x_3[0:x_3_len-1,:]\n",
    "y_3_len = len(y_3)\n",
    "target_3 = y_3[1:y_3_len]\n",
    "rf_regressor_3 = rf_regressor.fit(train_3,target_3)\n",
    "\n",
    "for sample in period_3_samples:\n",
    "    df_test = load_file_test(sample)\n",
    "    feature_df_test = get_features(df_test)\n",
    "    feature_df_test.drop(columns =['time'],inplace = True)\n",
    "    x_test = np.nan_to_num(feature_df_test.values)\n",
    "    x_test_len = x_test.shape[0]\n",
    "    test = x_test[0:x_test_len-1,:]\n",
    "    predicted_test = rf_regressor_3.predict(test)\n",
    "    print sample\n",
    "    print predicted_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
