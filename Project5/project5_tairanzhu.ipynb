{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuta\\Anaconda3\\envs\\py27\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the functions\n",
    "\n",
    "def load_file(files):\n",
    "    #info of total tweets\n",
    "    tw_info_total = []\n",
    "    for line in files :\n",
    "        #info of current tweets\n",
    "        tw_info = []\n",
    "        curr_tw = line\n",
    "        #time current tweet is posted\n",
    "        date = curr_tw['citation_date']\n",
    "        #follower of this user\n",
    "        foll = curr_tw['author']['followers']\n",
    "        #name of author\n",
    "        author = curr_tw['author']['name']\n",
    "        #number of retweets\n",
    "        re_tw = curr_tw['metrics']['citations']['total']\n",
    "        #store info as (date,author,follower,retweet_count)\n",
    "        tw_info.append(date)\n",
    "        tw_info.append(author)\n",
    "        tw_info.append(foll)\n",
    "        tw_info.append(re_tw)\n",
    "        tw_info_total.append(tw_info)\n",
    "    #convert to dataframe\n",
    "    df = pd.DataFrame(tw_info_total,columns=['time','author','follower','retweet'])\n",
    "    df = df.sort_values(by = 'time')\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_hour(time_stamp):\n",
    "    pst_tz = pytz.timezone('US/Pacific')\n",
    "    return (datetime.fromtimestamp(time_stamp, pst_tz)).hour\n",
    "\n",
    "def avg_rmse_lr(features,labels):\n",
    "    kf = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "    test_mse_list = []\n",
    "\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "        linearregression.fit(X_train, y_train)\n",
    "        \n",
    "        # Compute mse for test set\n",
    "        y_test_pred = linearregression.predict(X_test)\n",
    "        test_mse_list.append(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    avg_test_rmse = np.sqrt(np.mean(test_mse_list))\n",
    "\n",
    "    print ('RMSE for linear regression model is', avg_test_rmse)\n",
    "\n",
    "def avg_rmse_rf(features,labels):\n",
    "    kf = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "    test_mse_list = []\n",
    "\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "        rf_regressor.fit(X_train, y_train)\n",
    "        \n",
    "        # Compute mse for test set\n",
    "        y_test_pred = rf_regressor.predict(X_test)\n",
    "        test_mse_list.append(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    avg_test_rmse = np.sqrt(np.mean(test_mse_list))\n",
    "\n",
    "    print ('RMSE for Random Forest Regressor model is', avg_test_rmse)\n",
    "\n",
    "def avg_rmse_svm(features,labels):\n",
    "    kf = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "    test_mse_list = []\n",
    "\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        # Compute mse for test set\n",
    "        y_test_pred = svm.predict(X_test)\n",
    "        test_mse_list.append(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    avg_test_rmse = np.sqrt(np.mean(test_mse_list))\n",
    "\n",
    "    print ('RMSE for LinearSVR model is', avg_test_rmse)\n",
    "    \n",
    "    \n",
    "def get_features(df):\n",
    "    tw_tot = 0\n",
    "    retweet_tot = 0\n",
    "    foll_tot = 0\n",
    "    author_visited_dict = dict()\n",
    "    foll_max = 0\n",
    "    start_hour = get_hour(df.iloc[0,0])\n",
    "    \n",
    "    feature_list_tot = []\n",
    "    feature_list_curr = []\n",
    "    \n",
    "            \n",
    "    for index, row in df.iterrows():\n",
    "        curr_hour = get_hour(row['time'])\n",
    "        curr_author = row['author']\n",
    "        curr_foll = row['follower'] \n",
    "        curr_retweet = row['retweet']\n",
    "        \n",
    "        if curr_hour == start_hour:\n",
    "            tw_tot += 1\n",
    "            retweet_tot += curr_retweet\n",
    "            foll_max = max(foll_max,curr_foll)\n",
    "            #author visited or not\n",
    "            if (curr_author not in author_visited_dict):\n",
    "                foll_tot += curr_foll\n",
    "                author_visited_dict[curr_author] = True;\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            feature_list_curr.append(start_hour)\n",
    "            feature_list_curr.append(tw_tot)\n",
    "            feature_list_curr.append(retweet_tot)\n",
    "            feature_list_curr.append(foll_tot)\n",
    "            feature_list_curr.append(foll_max)\n",
    "            feature_list_tot.append(feature_list_curr)\n",
    "        \n",
    "            #setup the counters\n",
    "            tw_tot = 1\n",
    "            start_hour = curr_hour\n",
    "            retweet_tot = curr_retweet\n",
    "            foll_tot = curr_foll\n",
    "            foll_max = curr_foll\n",
    "            author_visited_dict.clear()\n",
    "            feature_list_curr = []\n",
    "        \n",
    "    feature_df = pd.DataFrame(feature_list_tot,columns=['time','tweets_total','retweets_total','followers_total','max_followers'])\n",
    "    return feature_df\n",
    "\n",
    "def cross_validation_3period(data):\n",
    "    # split tweets\n",
    "    time1 = 1422806400 #20150201 8:00am\n",
    "    time2 = 1422849600 #20150201 8:00pm\n",
    "\n",
    "    data_1 = []\n",
    "    data_2 = []\n",
    "    data_3 = []\n",
    "    for i in range(0,len(data)):\n",
    "        tweet = data[i]\n",
    "        time = tweet[\"firstpost_date\"]\n",
    "        if   time < time1: \n",
    "             data_1.append(tweet)\n",
    "        elif time >= time1 and time < time2: \n",
    "             data_2.append(tweet)\n",
    "        else: \n",
    "             data_3.append(tweet)\n",
    "\n",
    "    # first period\n",
    "    df_1 = load_file(data_1)\n",
    "    feature_df_1 = get_features(df_1)\n",
    "    y_1 = np.nan_to_num(feature_df_1['tweets_total'].values)\n",
    "    feature_df_1.drop(columns =['tweets_total'],inplace = True)\n",
    "    x_1 = np.nan_to_num(feature_df_1.values)\n",
    "\n",
    "    # rmse_1\n",
    "    print \"first period\"\n",
    "    avg_rmse_lr(x_1,y_1)\n",
    "    avg_rmse_rf(x_1,y_1)\n",
    "    avg_rmse_svm(x_1,y_1)\n",
    "    \n",
    "    # second period\n",
    "    df_2 = load_file(data_2)\n",
    "    feature_df_2 = get_features(df_2)\n",
    "    y_2 = np.nan_to_num(feature_df_2['tweets_total'].values)\n",
    "    feature_df_2.drop(columns =['tweets_total'],inplace = True)\n",
    "    x_2 = np.nan_to_num(feature_df_2.values)    \n",
    "\n",
    "    # rmse_2\n",
    "    print \"second period\"\n",
    "    avg_rmse_lr(x_2,y_2)\n",
    "    avg_rmse_rf(x_2,y_2)\n",
    "    avg_rmse_svm(x_2,y_2)\n",
    "    \n",
    "    \n",
    "    # third period\n",
    "    df_3 = load_file(data_3)\n",
    "    feature_df_3 = get_features(df_3)\n",
    "    y_3 = np.nan_to_num(feature_df_3['tweets_total'].values)\n",
    "    feature_df_3.drop(columns =['tweets_total'],inplace = True)\n",
    "    x_3 = np.nan_to_num(feature_df_3.values)    \n",
    "\n",
    "    # rmse_3\n",
    "    print \"third period\"\n",
    "    avg_rmse_lr(x_3,y_3)\n",
    "    avg_rmse_rf(x_3,y_3)\n",
    "    avg_rmse_svm(x_3,y_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files[0] => tweets_#gohawks.txt\n",
      "files[1] => tweets_#gopatriots.txt\n",
      "files[2] => tweets_#nfl.txt\n",
      "files[3] => tweets_#patriots.txt\n",
      "files[4] => tweets_#sb49.txt\n",
      "files[5] => tweets_#superbowl.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"tweet_data/\"\n",
    "\n",
    "files = [\"tweets_#gohawks.txt\", \"tweets_#gopatriots.txt\", \n",
    "        \"tweets_#nfl.txt\", \"tweets_#patriots.txt\", \n",
    "        \"tweets_#sb49.txt\", \"tweets_#superbowl.txt\"]\n",
    "\n",
    "for index, name in enumerate(files):\n",
    "    print (\"files[\" + str(index) + \"] => \" + name)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearregression = LinearRegression()\n",
    "rf_regressor = RandomForestRegressor(n_estimators=13,\n",
    "                             max_features=3,\n",
    "                             max_depth=11,\n",
    "                             bootstrap=True,\n",
    "                             oob_score=True,\n",
    "                             random_state=0)\n",
    "svm = LinearSVR(random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.4 (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag gohawks\n",
      "first period\n",
      "('RMSE for linear regression model is', 167.03768838768028)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuta\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\ensemble\\forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RMSE for Random Forest Regressor model is', 727.5054539499181)\n",
      "('RMSE for LinearSVR model is', 803.8749796798273)\n",
      "second period\n",
      "('RMSE for linear regression model is', 1630.8844284398092)\n",
      "('RMSE for Random Forest Regressor model is', 1596.6966920583197)\n",
      "('RMSE for LinearSVR model is', 2819.767432446993)\n",
      "third period\n",
      "('RMSE for linear regression model is', 110.48743810108999)\n",
      "('RMSE for Random Forest Regressor model is', 72.61467506221122)\n",
      "('RMSE for LinearSVR model is', 1004.4379343756998)\n"
     ]
    }
   ],
   "source": [
    "# hashtag tweets_#gohawks \n",
    "\n",
    "# load #gohawks\n",
    "f = open('tweet_data/tweets_#gohawks.txt')\n",
    "f_start = f.tell()\n",
    "f.seek(f_start)\n",
    "gohawks = []\n",
    "\n",
    "for line in f.readlines():\n",
    "    tweet = json.loads(line)\n",
    "    gohawks.append(tweet)\n",
    "\n",
    "print 'hashtag gohawks'\n",
    "cross_validation_3period(gohawks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag gopatriots\n",
      "first period\n",
      "('RMSE for linear regression model is', 27.738876380824657)\n",
      "('RMSE for Random Forest Regressor model is', 32.68739272532083)\n",
      "('RMSE for LinearSVR model is', 235.4174696983586)\n",
      "second period\n",
      "('RMSE for linear regression model is', 200.06407907757685)\n",
      "('RMSE for Random Forest Regressor model is', 482.0313536865456)\n",
      "('RMSE for LinearSVR model is', 4368.305756552469)\n",
      "third period\n",
      "('RMSE for linear regression model is', 10.841963798610175)\n",
      "('RMSE for Random Forest Regressor model is', 21.621744205433732)\n",
      "('RMSE for LinearSVR model is', 31.57203096725895)\n"
     ]
    }
   ],
   "source": [
    "# hashtag tweets_#gopatriots \n",
    "# load #gopatriots\n",
    "f = open('tweet_data/tweets_#gopatriots.txt')\n",
    "f_start = f.tell()\n",
    "f.seek(f_start)\n",
    "gopatriots = []\n",
    "\n",
    "for line in f.readlines():\n",
    "    tweet = json.loads(line)\n",
    "    gopatriots.append(tweet)\n",
    "\n",
    "print 'hashtag gopatriots'\n",
    "cross_validation_3period(gopatriots)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag nfl\n",
      "first period\n",
      "('RMSE for linear regression model is', 155.9752610964478)\n",
      "('RMSE for Random Forest Regressor model is', 224.45696125859274)\n",
      "('RMSE for LinearSVR model is', 1273.4563794096803)\n",
      "second period\n",
      "('RMSE for linear regression model is', 1227.4562012989693)\n",
      "('RMSE for Random Forest Regressor model is', 1258.7458206194854)\n",
      "('RMSE for LinearSVR model is', 6052.015182532136)\n",
      "third period\n",
      "('RMSE for linear regression model is', 169.898347617545)\n",
      "('RMSE for Random Forest Regressor model is', 132.92495049017492)\n",
      "('RMSE for LinearSVR model is', 1275.2003070593773)\n"
     ]
    }
   ],
   "source": [
    "# hashtag tweets_#nfl \n",
    "# load #nfl\n",
    "f = open('tweet_data/tweets_#nfl.txt')\n",
    "f_start = f.tell()\n",
    "f.seek(f_start)\n",
    "nfl = []\n",
    "\n",
    "for line in f.readlines():\n",
    "    tweet = json.loads(line)\n",
    "    nfl.append(tweet)\n",
    "\n",
    "print 'hashtag nfl'\n",
    "cross_validation_3period(nfl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag patriots\n",
      "first period\n",
      "('RMSE for linear regression model is', 252.2388884646137)\n",
      "('RMSE for Random Forest Regressor model is', 420.2640231172712)\n",
      "('RMSE for LinearSVR model is', 837.5325952326065)\n",
      "second period\n",
      "('RMSE for linear regression model is', 6739.312504509933)\n",
      "('RMSE for Random Forest Regressor model is', 4619.455209069529)\n",
      "('RMSE for LinearSVR model is', 15040.273369849503)\n",
      "third period\n",
      "('RMSE for linear regression model is', 149.74025062904835)\n",
      "('RMSE for Random Forest Regressor model is', 206.71995392030684)\n",
      "('RMSE for LinearSVR model is', 1926.2940234028515)\n"
     ]
    }
   ],
   "source": [
    "# hashtag tweets_#patriots \n",
    "# load #patriots\n",
    "f = open('tweet_data/tweets_#patriots.txt')\n",
    "f_start = f.tell()\n",
    "f.seek(f_start)\n",
    "patriots = []\n",
    "\n",
    "for line in f.readlines():\n",
    "    tweet = json.loads(line)\n",
    "    patriots.append(tweet)\n",
    "\n",
    "print 'hashtag patriots'\n",
    "cross_validation_3period(patriots) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag sb49\n",
      "first period\n",
      "('RMSE for linear regression model is', 140.24665457188496)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuta\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\ensemble\\forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RMSE for Random Forest Regressor model is', 123.15885969349496)\n",
      "('RMSE for LinearSVR model is', 8416.81182723944)\n",
      "second period\n",
      "('RMSE for linear regression model is', 20339.00615419173)\n",
      "('RMSE for Random Forest Regressor model is', 12320.226618078446)\n",
      "('RMSE for LinearSVR model is', 326388.1843666363)\n",
      "third period\n",
      "('RMSE for linear regression model is', 322.44149973391967)\n",
      "('RMSE for Random Forest Regressor model is', 479.58561606692797)\n",
      "('RMSE for LinearSVR model is', 12081.769207523153)\n"
     ]
    }
   ],
   "source": [
    "# hashtag tweets_#sb49 \n",
    "# load #sb49\n",
    "f = open('tweet_data/tweets_#sb49.txt')\n",
    "f_start = f.tell()\n",
    "f.seek(f_start)\n",
    "sb49 = []\n",
    "\n",
    "for line in f.readlines():\n",
    "    tweet = json.loads(line)\n",
    "    sb49.append(tweet)\n",
    "\n",
    "print 'hashtag sb49'\n",
    "cross_validation_3period(sb49) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a0d0162c13f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0msuperbowl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zhuta\\Anaconda3\\envs\\py27\\lib\\json\\__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zhuta\\Anaconda3\\envs\\py27\\lib\\json\\decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \"\"\"\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zhuta\\Anaconda3\\envs\\py27\\lib\\json\\decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \"\"\"\n\u001b[0;32m    379\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No JSON object could be decoded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hashtag tweets_#superbowl \n",
    "# load #superbowl\n",
    "f = open('tweet_data/tweets_#superbowl.txt')\n",
    "f_start = f.tell()\n",
    "f.seek(f_start)\n",
    "superbowl = []\n",
    "\n",
    "for line in f.readlines():\n",
    "    tweet = json.loads(line)\n",
    "    superbowl.append(tweet)\n",
    "\n",
    "print 'hashtag superbowl'\n",
    "cross_validation_3period(superbowl) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (iiï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all hashtags \n",
    "# By observing the top 3 features of each hashtag, we decided to choose 4 features, excluding 'time', to predict and fit the model\n",
    "# load #gohawks\n",
    "allhashtag = ['tweets_#gohawks.txt','tweets_#gopatriots.txt','tweets_#nfl.txt',\n",
    "              'tweets_#patriots.txt','tweets_#sb49.txt','tweets_#superbowl.txt']\n",
    "\n",
    "for hashtag in allhashtag:\n",
    "    f = open ('tweet_data/' + hashtag)\n",
    "    f_start = f.tell()\n",
    "    f.seek(f_start)\n",
    "    all_hashtag = []\n",
    "\n",
    "    for line in f.readlines():\n",
    "        tweet = json.loads(line)\n",
    "        all_hashtag.append(tweet)\n",
    "\n",
    "print 'all hashtag'\n",
    "cross_validation_3period(all_hashtag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first period testing data\n",
    "period_1_samples = ['sample1_period1','sample4_period1','sample5_period1','sample8_period1']\n",
    "period_2_samples = ['sample2_period2','sample6_period2','sample9_period2']\n",
    "period_3_samples = ['sample3_period3','sample7_period3','sample10_period3']\n",
    "\n",
    "for sample in period_1_samples\n",
    "    f = open ('test_data/' + sample)\n",
    "    f_start = f.tell()\n",
    "    f.seek(f_start)\n",
    "    period_1_test = []\n",
    "\n",
    "    for line in f.readlines():\n",
    "        tweet = json.loads(line)\n",
    "        period_1_test.append(tweet)\n",
    "        \n",
    "for sample in period_2_samples\n",
    "    f = open ('test_data/' + sample)\n",
    "    f_start = f.tell()\n",
    "    f.seek(f_start)\n",
    "    period_2_test = []\n",
    "\n",
    "    for line in f.readlines():\n",
    "        tweet = json.loads(line)\n",
    "        period_2_test.append(tweet)\n",
    "        \n",
    "for sample in period_3_samples\n",
    "    f = open ('test_data/' + sample)\n",
    "    f_start = f.tell()\n",
    "    f.seek(f_start)\n",
    "    period_3_test = []\n",
    "\n",
    "    for line in f.readlines():\n",
    "        tweet = json.loads(line)\n",
    "        period_3_test.append(tweet)\n",
    "        \n",
    "# split tweets\n",
    "time1 = 1422806400 #20150201 8:00am\n",
    "time2 = 1422849600 #20150201 8:00pm\n",
    "\n",
    "data_1 = []\n",
    "data_2 = []\n",
    "data_3 = []\n",
    "for i in range(0,len(all_hashtag)):\n",
    "    tweet = all_hashtag[i]\n",
    "    time = tweet[\"firstpost_date\"]\n",
    "    if   time < time1: \n",
    "         data_1.append(tweet)\n",
    "    elif time >= time1 and time < time2: \n",
    "         data_2.append(tweet)\n",
    "    else: \n",
    "         data_3.append(tweet)\n",
    "\n",
    "df_1 = load_file(data_1)\n",
    "feature_df_1 = get_features(df_1)\n",
    "y_1 = np.nan_to_num(feature_df_1['tweets_total'].values)\n",
    "feature_df_1.drop(columns =['tweets_total'],inplace = True)\n",
    "x_1 = np.nan_to_num(feature_df_1.values)\n",
    "rf_regressor.fit(x_1, y_1)\n",
    "\n",
    "df_test_1 = load_file(period_1_test)\n",
    "feature_df_test_1 = get_features(df_test_1)\n",
    "y_test_1 = np.nan_to_num(feature_df_test_1['tweets_total'].values)\n",
    "feature_df_test_1.drop(columns =['tweets_total'],inplace = True)\n",
    "x_test_1 = np.nan_to_num(feature_df_test_1.values)\n",
    "\n",
    "#calculating rmse for first period test data\n",
    "y_test_pred_1 = rf_regressor.predict(x_test_1)\n",
    "rmse_test_1 = np.sqrt(mean_squared_error(y_test_1, y_test_pred))\n",
    "\n",
    "\n",
    "df_2 = load_file(data_2)\n",
    "feature_df_2 = get_features(df_2)\n",
    "y_2 = np.nan_to_num(feature_df_2['tweets_total'].values)\n",
    "feature_df_2.drop(columns =['tweets_total'],inplace = True)\n",
    "x_2 = np.nan_to_num(feature_df_2.values)\n",
    "rf_regressor.fit(x_2, y_2)\n",
    "\n",
    "df_test_2 = load_file(period_2_test)\n",
    "feature_df_test_2 = get_features(df_test_2)\n",
    "y_test_2 = np.nan_to_num(feature_df_test_2['tweets_total'].values)\n",
    "feature_df_test_2.drop(columns =['tweets_total'],inplace = True)\n",
    "x_test_2 = np.nan_to_num(feature_df_test_2.values)\n",
    "\n",
    "#calculating rmse for second period test data\n",
    "y_test_pred_2 = rf_regressor.predict(x_test_2)\n",
    "rmse_test_2 = np.sqrt(mean_squared_error(y_test_2, y_test_pred_2))\n",
    "\n",
    "\n",
    "df_3 = load_file(data_3)\n",
    "feature_df_3 = get_features(df_3)\n",
    "y_3 = np.nan_to_num(feature_df_3['tweets_total'].values)\n",
    "feature_df_3.drop(columns =['tweets_total'],inplace = True)\n",
    "x_3 = np.nan_to_num(feature_df_3.values)\n",
    "rf_regressor.fit(x_3, y_3)\n",
    "\n",
    "df_test_3 = load_file(period_3_test)\n",
    "feature_df_test_3 = get_features(df_test_3)\n",
    "y_test_3 = np.nan_to_num(feature_df_test_3['tweets_total'].values)\n",
    "feature_df_test_3.drop(columns =['tweets_total'],inplace = True)\n",
    "x_test_3 = np.nan_to_num(feature_df_test_3.values)\n",
    "\n",
    "#calculating rmse for second period test data\n",
    "y_test_pred_3 = rf_regressor.predict(x_test_3)\n",
    "rmse_test_3 = np.sqrt(mean_squared_error(y_test_3, y_test_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
