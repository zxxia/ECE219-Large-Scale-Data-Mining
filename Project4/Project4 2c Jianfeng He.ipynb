{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels[0] => Week #\n",
      "labels[1] => Day of Week\n",
      "labels[2] => Backup Start Time - Hour of Day\n",
      "labels[3] => Work-Flow-ID\n",
      "labels[4] => File Name\n",
      "labels[5] => Size of Backup (GB)\n",
      "labels[6] => Backup Time (hour)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "import scipy as sp\n",
    "Data = pd.read_csv(\"network_backup_dataset.csv\")\n",
    "labels = Data.columns\n",
    "for i, v in enumerate(labels):\n",
    "    print \"labels[\" + str(i) + \"] => \" + v\n",
    "\n",
    "def convert_OneDimension(label, data):\n",
    "    \n",
    "    if label == labels[1]:\n",
    "        days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        for day, number in zip(days, range(len(days))):\n",
    "            data = data.replace({'Day of Week': {day : number}})\n",
    "        return data\n",
    "    \n",
    "    elif label == labels[3]:\n",
    "        work_flow_ids = ['work_flow_0', 'work_flow_1','work_flow_2','work_flow_3', 'work_flow_4']\n",
    "        for work_flow, number in zip(work_flow_ids, range(len(work_flow_ids))):\n",
    "            data = data.replace({'Work-Flow-ID' : {work_flow : number}})\n",
    "        return data\n",
    "    \n",
    "    elif label == labels[4]:\n",
    "        uniqueFiles = ['File_{0}'.format(s) for s in range(len((pd.unique(Data['File Name']))))]\n",
    "        for file_name, number in zip(uniqueFiles,range(len(uniqueFiles))):\n",
    "            data = data.replace({'File Name' : {file_name : number}})\n",
    "        return data      \n",
    "    else:\n",
    "        return data\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    Data = convert_OneDimension(labels[i], Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.12993761\n",
      "Iteration 2, loss = 0.02169919\n",
      "Iteration 3, loss = 0.01082054\n",
      "Iteration 4, loss = 0.00694757\n",
      "Iteration 5, loss = 0.00553834\n",
      "Iteration 6, loss = 0.00498354\n",
      "Iteration 7, loss = 0.00472313\n",
      "Iteration 8, loss = 0.00456536\n",
      "Iteration 9, loss = 0.00444935\n",
      "Iteration 10, loss = 0.00435965\n",
      "Iteration 11, loss = 0.00428391\n",
      "Iteration 12, loss = 0.00422263\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03872887\n",
      "Iteration 2, loss = 0.01142257\n",
      "Iteration 3, loss = 0.00690440\n",
      "Iteration 4, loss = 0.00541519\n",
      "Iteration 5, loss = 0.00469017\n",
      "Iteration 6, loss = 0.00424432\n",
      "Iteration 7, loss = 0.00390976\n",
      "Iteration 8, loss = 0.00363582\n",
      "Iteration 9, loss = 0.00338748\n",
      "Iteration 10, loss = 0.00314167\n",
      "Iteration 11, loss = 0.00289909\n",
      "Iteration 12, loss = 0.00265982\n",
      "Iteration 13, loss = 0.00242697\n",
      "Iteration 14, loss = 0.00220748\n",
      "Iteration 15, loss = 0.00199501\n",
      "Iteration 16, loss = 0.00178233\n",
      "Iteration 17, loss = 0.00161482\n",
      "Iteration 18, loss = 0.00146350\n",
      "Iteration 19, loss = 0.00132225\n",
      "Iteration 20, loss = 0.00119538\n",
      "Iteration 21, loss = 0.00107141\n",
      "Iteration 22, loss = 0.00095941\n",
      "Iteration 23, loss = 0.00084959\n",
      "Iteration 24, loss = 0.00076275\n",
      "Iteration 25, loss = 0.00068539\n",
      "Iteration 26, loss = 0.00062074\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03795701\n",
      "Iteration 2, loss = 0.00933668\n",
      "Iteration 3, loss = 0.00694282\n",
      "Iteration 4, loss = 0.00566877\n",
      "Iteration 5, loss = 0.00471890\n",
      "Iteration 6, loss = 0.00400867\n",
      "Iteration 7, loss = 0.00345161\n",
      "Iteration 8, loss = 0.00301375\n",
      "Iteration 9, loss = 0.00267186\n",
      "Iteration 10, loss = 0.00239338\n",
      "Iteration 11, loss = 0.00218877\n",
      "Iteration 12, loss = 0.00201499\n",
      "Iteration 13, loss = 0.00187365\n",
      "Iteration 14, loss = 0.00175101\n",
      "Iteration 15, loss = 0.00164348\n",
      "Iteration 16, loss = 0.00154238\n",
      "Iteration 17, loss = 0.00143987\n",
      "Iteration 18, loss = 0.00134151\n",
      "Iteration 19, loss = 0.00123969\n",
      "Iteration 20, loss = 0.00113927\n",
      "Iteration 21, loss = 0.00104379\n",
      "Iteration 22, loss = 0.00095615\n",
      "Iteration 23, loss = 0.00086277\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01914161\n",
      "Iteration 2, loss = 0.00557446\n",
      "Iteration 3, loss = 0.00412298\n",
      "Iteration 4, loss = 0.00316640\n",
      "Iteration 5, loss = 0.00254665\n",
      "Iteration 6, loss = 0.00215915\n",
      "Iteration 7, loss = 0.00187848\n",
      "Iteration 8, loss = 0.00166729\n",
      "Iteration 9, loss = 0.00149420\n",
      "Iteration 10, loss = 0.00134534\n",
      "Iteration 11, loss = 0.00120528\n",
      "Iteration 12, loss = 0.00107274\n",
      "Iteration 13, loss = 0.00095831\n",
      "Iteration 14, loss = 0.00084649\n",
      "Iteration 15, loss = 0.00074285\n",
      "Iteration 16, loss = 0.00064690\n",
      "Iteration 17, loss = 0.00056856\n",
      "Iteration 18, loss = 0.00049383\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00677060\n",
      "Iteration 2, loss = 0.00297230\n",
      "Iteration 3, loss = 0.00200584\n",
      "Iteration 4, loss = 0.00159017\n",
      "Iteration 5, loss = 0.00130511\n",
      "Iteration 6, loss = 0.00109104\n",
      "Iteration 7, loss = 0.00087639\n",
      "Iteration 8, loss = 0.00071674\n",
      "Iteration 9, loss = 0.00057206\n",
      "Iteration 10, loss = 0.00044491\n",
      "Iteration 11, loss = 0.00034573\n",
      "Iteration 12, loss = 0.00028254\n",
      "Iteration 13, loss = 0.00023770\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00468966\n",
      "Iteration 2, loss = 0.00195494\n",
      "Iteration 3, loss = 0.00140249\n",
      "Iteration 4, loss = 0.00105019\n",
      "Iteration 5, loss = 0.00070706\n",
      "Iteration 6, loss = 0.00045599\n",
      "Iteration 7, loss = 0.00031964\n",
      "Iteration 8, loss = 0.00024762\n",
      "Iteration 9, loss = 0.00021716\n",
      "Iteration 10, loss = 0.00019340\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00390089\n",
      "Iteration 2, loss = 0.00154519\n",
      "Iteration 3, loss = 0.00104063\n",
      "Iteration 4, loss = 0.00063245\n",
      "Iteration 5, loss = 0.00033915\n",
      "Iteration 6, loss = 0.00020922\n",
      "Iteration 7, loss = 0.00016801\n",
      "Iteration 8, loss = 0.00014837\n",
      "Iteration 9, loss = 0.00013023\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.07991929\n",
      "Iteration 2, loss = 0.02084734\n",
      "Iteration 3, loss = 0.01018374\n",
      "Iteration 4, loss = 0.00647519\n",
      "Iteration 5, loss = 0.00515164\n",
      "Iteration 6, loss = 0.00462152\n",
      "Iteration 7, loss = 0.00433632\n",
      "Iteration 8, loss = 0.00413310\n",
      "Iteration 9, loss = 0.00397208\n",
      "Iteration 10, loss = 0.00383771\n",
      "Iteration 11, loss = 0.00371394\n",
      "Iteration 12, loss = 0.00359329\n",
      "Iteration 13, loss = 0.00347810\n",
      "Iteration 14, loss = 0.00335896\n",
      "Iteration 15, loss = 0.00321784\n",
      "Iteration 16, loss = 0.00308097\n",
      "Iteration 17, loss = 0.00296190\n",
      "Iteration 18, loss = 0.00285233\n",
      "Iteration 19, loss = 0.00277217\n",
      "Iteration 20, loss = 0.00269985\n",
      "Iteration 21, loss = 0.00264230\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02362994\n",
      "Iteration 2, loss = 0.00869503\n",
      "Iteration 3, loss = 0.00547108\n",
      "Iteration 4, loss = 0.00442118\n",
      "Iteration 5, loss = 0.00383725\n",
      "Iteration 6, loss = 0.00336792\n",
      "Iteration 7, loss = 0.00294656\n",
      "Iteration 8, loss = 0.00259579\n",
      "Iteration 9, loss = 0.00232850\n",
      "Iteration 10, loss = 0.00212656\n",
      "Iteration 11, loss = 0.00198356\n",
      "Iteration 12, loss = 0.00187299\n",
      "Iteration 13, loss = 0.00177920\n",
      "Iteration 14, loss = 0.00167251\n",
      "Iteration 15, loss = 0.00157330\n",
      "Iteration 16, loss = 0.00146991\n",
      "Iteration 17, loss = 0.00137174\n",
      "Iteration 18, loss = 0.00125685\n",
      "Iteration 19, loss = 0.00113986\n",
      "Iteration 20, loss = 0.00101393\n",
      "Iteration 21, loss = 0.00091783\n",
      "Iteration 22, loss = 0.00083469\n",
      "Iteration 23, loss = 0.00077394\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01433389\n",
      "Iteration 2, loss = 0.00533648\n",
      "Iteration 3, loss = 0.00403546\n",
      "Iteration 4, loss = 0.00333417\n",
      "Iteration 5, loss = 0.00282980\n",
      "Iteration 6, loss = 0.00240118\n",
      "Iteration 7, loss = 0.00203413\n",
      "Iteration 8, loss = 0.00174990\n",
      "Iteration 9, loss = 0.00150773\n",
      "Iteration 10, loss = 0.00132579\n",
      "Iteration 11, loss = 0.00118172\n",
      "Iteration 12, loss = 0.00106236\n",
      "Iteration 13, loss = 0.00095571\n",
      "Iteration 14, loss = 0.00084603\n",
      "Iteration 15, loss = 0.00074829\n",
      "Iteration 16, loss = 0.00065403\n",
      "Iteration 17, loss = 0.00057394\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01097139\n",
      "Iteration 2, loss = 0.00462396\n",
      "Iteration 3, loss = 0.00299589\n",
      "Iteration 4, loss = 0.00216784\n",
      "Iteration 5, loss = 0.00169505\n",
      "Iteration 6, loss = 0.00137436\n",
      "Iteration 7, loss = 0.00112562\n",
      "Iteration 8, loss = 0.00090946\n",
      "Iteration 9, loss = 0.00073571\n",
      "Iteration 10, loss = 0.00058583\n",
      "Iteration 11, loss = 0.00046900\n",
      "Iteration 12, loss = 0.00038494\n",
      "Iteration 13, loss = 0.00032971\n",
      "Iteration 14, loss = 0.00028623\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00798555\n",
      "Iteration 2, loss = 0.00318188\n",
      "Iteration 3, loss = 0.00218674\n",
      "Iteration 4, loss = 0.00169757\n",
      "Iteration 5, loss = 0.00138244\n",
      "Iteration 6, loss = 0.00113543\n",
      "Iteration 7, loss = 0.00088634\n",
      "Iteration 8, loss = 0.00067527\n",
      "Iteration 9, loss = 0.00050383\n",
      "Iteration 10, loss = 0.00038011\n",
      "Iteration 11, loss = 0.00030075\n",
      "Iteration 12, loss = 0.00025728\n",
      "Iteration 13, loss = 0.00022540\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00476621\n",
      "Iteration 2, loss = 0.00198690\n",
      "Iteration 3, loss = 0.00143945\n",
      "Iteration 4, loss = 0.00103917\n",
      "Iteration 5, loss = 0.00071411\n",
      "Iteration 6, loss = 0.00045844\n",
      "Iteration 7, loss = 0.00030116\n",
      "Iteration 8, loss = 0.00024035\n",
      "Iteration 9, loss = 0.00019951\n",
      "Iteration 10, loss = 0.00017678\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00394826\n",
      "Iteration 2, loss = 0.00152948\n",
      "Iteration 3, loss = 0.00108979\n",
      "Iteration 4, loss = 0.00067975\n",
      "Iteration 5, loss = 0.00036982\n",
      "Iteration 6, loss = 0.00023481\n",
      "Iteration 7, loss = 0.00018029\n",
      "Iteration 8, loss = 0.00015731\n",
      "Iteration 9, loss = 0.00014653\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04041732\n",
      "Iteration 2, loss = 0.01505725\n",
      "Iteration 3, loss = 0.00934471\n",
      "Iteration 4, loss = 0.00688458\n",
      "Iteration 5, loss = 0.00570754\n",
      "Iteration 6, loss = 0.00505228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.00459554\n",
      "Iteration 8, loss = 0.00423666\n",
      "Iteration 9, loss = 0.00393021\n",
      "Iteration 10, loss = 0.00366401\n",
      "Iteration 11, loss = 0.00341466\n",
      "Iteration 12, loss = 0.00318695\n",
      "Iteration 13, loss = 0.00298082\n",
      "Iteration 14, loss = 0.00280799\n",
      "Iteration 15, loss = 0.00265659\n",
      "Iteration 16, loss = 0.00252649\n",
      "Iteration 17, loss = 0.00241361\n",
      "Iteration 18, loss = 0.00231222\n",
      "Iteration 19, loss = 0.00222659\n",
      "Iteration 20, loss = 0.00215442\n",
      "Iteration 21, loss = 0.00209472\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.12175529\n",
      "Iteration 2, loss = 0.02743882\n",
      "Iteration 3, loss = 0.01188667\n",
      "Iteration 4, loss = 0.00805227\n",
      "Iteration 5, loss = 0.00682523\n",
      "Iteration 6, loss = 0.00609701\n",
      "Iteration 7, loss = 0.00552599\n",
      "Iteration 8, loss = 0.00505177\n",
      "Iteration 9, loss = 0.00463991\n",
      "Iteration 10, loss = 0.00428489\n",
      "Iteration 11, loss = 0.00397235\n",
      "Iteration 12, loss = 0.00367673\n",
      "Iteration 13, loss = 0.00338650\n",
      "Iteration 14, loss = 0.00311883\n",
      "Iteration 15, loss = 0.00287032\n",
      "Iteration 16, loss = 0.00261669\n",
      "Iteration 17, loss = 0.00238304\n",
      "Iteration 18, loss = 0.00217421\n",
      "Iteration 19, loss = 0.00199673\n",
      "Iteration 20, loss = 0.00184388\n",
      "Iteration 21, loss = 0.00171094\n",
      "Iteration 22, loss = 0.00159538\n",
      "Iteration 23, loss = 0.00149257\n",
      "Iteration 24, loss = 0.00139583\n",
      "Iteration 25, loss = 0.00130004\n",
      "Iteration 26, loss = 0.00121351\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03227634\n",
      "Iteration 2, loss = 0.00783068\n",
      "Iteration 3, loss = 0.00569531\n",
      "Iteration 4, loss = 0.00461462\n",
      "Iteration 5, loss = 0.00378245\n",
      "Iteration 6, loss = 0.00315350\n",
      "Iteration 7, loss = 0.00268643\n",
      "Iteration 8, loss = 0.00235581\n",
      "Iteration 9, loss = 0.00211149\n",
      "Iteration 10, loss = 0.00192154\n",
      "Iteration 11, loss = 0.00177003\n",
      "Iteration 12, loss = 0.00164588\n",
      "Iteration 13, loss = 0.00152654\n",
      "Iteration 14, loss = 0.00143017\n",
      "Iteration 15, loss = 0.00134149\n",
      "Iteration 16, loss = 0.00126564\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01281608\n",
      "Iteration 2, loss = 0.00485749\n",
      "Iteration 3, loss = 0.00353919\n",
      "Iteration 4, loss = 0.00270062\n",
      "Iteration 5, loss = 0.00217498\n",
      "Iteration 6, loss = 0.00183446\n",
      "Iteration 7, loss = 0.00160014\n",
      "Iteration 8, loss = 0.00140591\n",
      "Iteration 9, loss = 0.00123907\n",
      "Iteration 10, loss = 0.00108499\n",
      "Iteration 11, loss = 0.00094111\n",
      "Iteration 12, loss = 0.00079878\n",
      "Iteration 13, loss = 0.00066864\n",
      "Iteration 14, loss = 0.00054362\n",
      "Iteration 15, loss = 0.00045078\n",
      "Iteration 16, loss = 0.00038200\n",
      "Iteration 17, loss = 0.00033418\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00741705\n",
      "Iteration 2, loss = 0.00294102\n",
      "Iteration 3, loss = 0.00195220\n",
      "Iteration 4, loss = 0.00150475\n",
      "Iteration 5, loss = 0.00118420\n",
      "Iteration 6, loss = 0.00091978\n",
      "Iteration 7, loss = 0.00069281\n",
      "Iteration 8, loss = 0.00051614\n",
      "Iteration 9, loss = 0.00039369\n",
      "Iteration 10, loss = 0.00031483\n",
      "Iteration 11, loss = 0.00026660\n",
      "Iteration 12, loss = 0.00023471\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00710694\n",
      "Iteration 2, loss = 0.00267068\n",
      "Iteration 3, loss = 0.00176854\n",
      "Iteration 4, loss = 0.00137162\n",
      "Iteration 5, loss = 0.00105393\n",
      "Iteration 6, loss = 0.00077170\n",
      "Iteration 7, loss = 0.00052969\n",
      "Iteration 8, loss = 0.00036983\n",
      "Iteration 9, loss = 0.00028348\n",
      "Iteration 10, loss = 0.00023633\n",
      "Iteration 11, loss = 0.00021115\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00288101\n",
      "Iteration 2, loss = 0.00133971\n",
      "Iteration 3, loss = 0.00089169\n",
      "Iteration 4, loss = 0.00050543\n",
      "Iteration 5, loss = 0.00026639\n",
      "Iteration 6, loss = 0.00018768\n",
      "Iteration 7, loss = 0.00015808\n",
      "Iteration 8, loss = 0.00014607\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.35241277\n",
      "Iteration 2, loss = 0.08395804\n",
      "Iteration 3, loss = 0.02415798\n",
      "Iteration 4, loss = 0.01414543\n",
      "Iteration 5, loss = 0.00946766\n",
      "Iteration 6, loss = 0.00708781\n",
      "Iteration 7, loss = 0.00585218\n",
      "Iteration 8, loss = 0.00519467\n",
      "Iteration 9, loss = 0.00483972\n",
      "Iteration 10, loss = 0.00463582\n",
      "Iteration 11, loss = 0.00450575\n",
      "Iteration 12, loss = 0.00441370\n",
      "Iteration 13, loss = 0.00434516\n",
      "Iteration 14, loss = 0.00428989\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01794051\n",
      "Iteration 2, loss = 0.00711200\n",
      "Iteration 3, loss = 0.00505264\n",
      "Iteration 4, loss = 0.00412434\n",
      "Iteration 5, loss = 0.00359145\n",
      "Iteration 6, loss = 0.00326215\n",
      "Iteration 7, loss = 0.00300956\n",
      "Iteration 8, loss = 0.00278173\n",
      "Iteration 9, loss = 0.00256129\n",
      "Iteration 10, loss = 0.00235209\n",
      "Iteration 11, loss = 0.00216212\n",
      "Iteration 12, loss = 0.00200027\n",
      "Iteration 13, loss = 0.00186305\n",
      "Iteration 14, loss = 0.00175799\n",
      "Iteration 15, loss = 0.00166870\n",
      "Iteration 16, loss = 0.00159912\n",
      "Iteration 17, loss = 0.00154523\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02243506\n",
      "Iteration 2, loss = 0.00709077\n",
      "Iteration 3, loss = 0.00507875\n",
      "Iteration 4, loss = 0.00407866\n",
      "Iteration 5, loss = 0.00337730\n",
      "Iteration 6, loss = 0.00283429\n",
      "Iteration 7, loss = 0.00237936\n",
      "Iteration 8, loss = 0.00201519\n",
      "Iteration 9, loss = 0.00174577\n",
      "Iteration 10, loss = 0.00153646\n",
      "Iteration 11, loss = 0.00136378\n",
      "Iteration 12, loss = 0.00121933\n",
      "Iteration 13, loss = 0.00108965\n",
      "Iteration 14, loss = 0.00097249\n",
      "Iteration 15, loss = 0.00087312\n",
      "Iteration 16, loss = 0.00078018\n",
      "Iteration 17, loss = 0.00069045\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01430087\n",
      "Iteration 2, loss = 0.00540348\n",
      "Iteration 3, loss = 0.00400214\n",
      "Iteration 4, loss = 0.00310030\n",
      "Iteration 5, loss = 0.00252355\n",
      "Iteration 6, loss = 0.00214480\n",
      "Iteration 7, loss = 0.00189498\n",
      "Iteration 8, loss = 0.00169686\n",
      "Iteration 9, loss = 0.00153517\n",
      "Iteration 10, loss = 0.00139749\n",
      "Iteration 11, loss = 0.00125957\n",
      "Iteration 12, loss = 0.00113704\n",
      "Iteration 13, loss = 0.00101706\n",
      "Iteration 14, loss = 0.00090449\n",
      "Iteration 15, loss = 0.00079016\n",
      "Iteration 16, loss = 0.00068053\n",
      "Iteration 17, loss = 0.00057575\n",
      "Iteration 18, loss = 0.00048491\n",
      "Iteration 19, loss = 0.00041301\n",
      "Iteration 20, loss = 0.00035030\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00796412\n",
      "Iteration 2, loss = 0.00316846\n",
      "Iteration 3, loss = 0.00213754\n",
      "Iteration 4, loss = 0.00167459\n",
      "Iteration 5, loss = 0.00139251\n",
      "Iteration 6, loss = 0.00114631\n",
      "Iteration 7, loss = 0.00094268\n",
      "Iteration 8, loss = 0.00075133\n",
      "Iteration 9, loss = 0.00058749\n",
      "Iteration 10, loss = 0.00044847\n",
      "Iteration 11, loss = 0.00034360\n",
      "Iteration 12, loss = 0.00027999\n",
      "Iteration 13, loss = 0.00024171\n",
      "Iteration 14, loss = 0.00021379\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00737496\n",
      "Iteration 2, loss = 0.00255213\n",
      "Iteration 3, loss = 0.00172716\n",
      "Iteration 4, loss = 0.00134527\n",
      "Iteration 5, loss = 0.00104286\n",
      "Iteration 6, loss = 0.00077232\n",
      "Iteration 7, loss = 0.00053293\n",
      "Iteration 8, loss = 0.00036938\n",
      "Iteration 9, loss = 0.00027910\n",
      "Iteration 10, loss = 0.00023265\n",
      "Iteration 11, loss = 0.00020749\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00503777\n",
      "Iteration 2, loss = 0.00169507\n",
      "Iteration 3, loss = 0.00113579\n",
      "Iteration 4, loss = 0.00074248\n",
      "Iteration 5, loss = 0.00041152\n",
      "Iteration 6, loss = 0.00025021\n",
      "Iteration 7, loss = 0.00019064\n",
      "Iteration 8, loss = 0.00016130\n",
      "Iteration 9, loss = 0.00014388\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03926705\n",
      "Iteration 2, loss = 0.01304992\n",
      "Iteration 3, loss = 0.00826329\n",
      "Iteration 4, loss = 0.00621713\n",
      "Iteration 5, loss = 0.00516124\n",
      "Iteration 6, loss = 0.00456634\n",
      "Iteration 7, loss = 0.00422185\n",
      "Iteration 8, loss = 0.00400152\n",
      "Iteration 9, loss = 0.00382858\n",
      "Iteration 10, loss = 0.00366738\n",
      "Iteration 11, loss = 0.00347489\n",
      "Iteration 12, loss = 0.00326223\n",
      "Iteration 13, loss = 0.00307083\n",
      "Iteration 14, loss = 0.00288616\n",
      "Iteration 15, loss = 0.00270035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.00253041\n",
      "Iteration 17, loss = 0.00237769\n",
      "Iteration 18, loss = 0.00223095\n",
      "Iteration 19, loss = 0.00210797\n",
      "Iteration 20, loss = 0.00198787\n",
      "Iteration 21, loss = 0.00188201\n",
      "Iteration 22, loss = 0.00178344\n",
      "Iteration 23, loss = 0.00169868\n",
      "Iteration 24, loss = 0.00160424\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02531834\n",
      "Iteration 2, loss = 0.00859150\n",
      "Iteration 3, loss = 0.00581597\n",
      "Iteration 4, loss = 0.00487019\n",
      "Iteration 5, loss = 0.00434276\n",
      "Iteration 6, loss = 0.00398939\n",
      "Iteration 7, loss = 0.00373978\n",
      "Iteration 8, loss = 0.00354219\n",
      "Iteration 9, loss = 0.00335426\n",
      "Iteration 10, loss = 0.00315350\n",
      "Iteration 11, loss = 0.00294754\n",
      "Iteration 12, loss = 0.00275280\n",
      "Iteration 13, loss = 0.00256348\n",
      "Iteration 14, loss = 0.00237714\n",
      "Iteration 15, loss = 0.00220979\n",
      "Iteration 16, loss = 0.00204851\n",
      "Iteration 17, loss = 0.00188547\n",
      "Iteration 18, loss = 0.00174234\n",
      "Iteration 19, loss = 0.00159996\n",
      "Iteration 20, loss = 0.00147364\n",
      "Iteration 21, loss = 0.00135011\n",
      "Iteration 22, loss = 0.00125404\n",
      "Iteration 23, loss = 0.00117673\n",
      "Iteration 24, loss = 0.00110535\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02374776\n",
      "Iteration 2, loss = 0.00742925\n",
      "Iteration 3, loss = 0.00566275\n",
      "Iteration 4, loss = 0.00465864\n",
      "Iteration 5, loss = 0.00394965\n",
      "Iteration 6, loss = 0.00341129\n",
      "Iteration 7, loss = 0.00297361\n",
      "Iteration 8, loss = 0.00260346\n",
      "Iteration 9, loss = 0.00231512\n",
      "Iteration 10, loss = 0.00209457\n",
      "Iteration 11, loss = 0.00190494\n",
      "Iteration 12, loss = 0.00177519\n",
      "Iteration 13, loss = 0.00165256\n",
      "Iteration 14, loss = 0.00154696\n",
      "Iteration 15, loss = 0.00145351\n",
      "Iteration 16, loss = 0.00135886\n",
      "Iteration 17, loss = 0.00126849\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03003189\n",
      "Iteration 2, loss = 0.00680150\n",
      "Iteration 3, loss = 0.00519935\n",
      "Iteration 4, loss = 0.00412949\n",
      "Iteration 5, loss = 0.00331281\n",
      "Iteration 6, loss = 0.00271621\n",
      "Iteration 7, loss = 0.00230777\n",
      "Iteration 8, loss = 0.00201967\n",
      "Iteration 9, loss = 0.00180736\n",
      "Iteration 10, loss = 0.00164159\n",
      "Iteration 11, loss = 0.00149581\n",
      "Iteration 12, loss = 0.00136496\n",
      "Iteration 13, loss = 0.00123823\n",
      "Iteration 14, loss = 0.00112164\n",
      "Iteration 15, loss = 0.00100085\n",
      "Iteration 16, loss = 0.00088171\n",
      "Iteration 17, loss = 0.00077538\n",
      "Iteration 18, loss = 0.00067345\n",
      "Iteration 19, loss = 0.00058582\n",
      "Iteration 20, loss = 0.00050740\n",
      "Iteration 21, loss = 0.00044386\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00741736\n",
      "Iteration 2, loss = 0.00295091\n",
      "Iteration 3, loss = 0.00206437\n",
      "Iteration 4, loss = 0.00164661\n",
      "Iteration 5, loss = 0.00135849\n",
      "Iteration 6, loss = 0.00115167\n",
      "Iteration 7, loss = 0.00096308\n",
      "Iteration 8, loss = 0.00076787\n",
      "Iteration 9, loss = 0.00059869\n",
      "Iteration 10, loss = 0.00046346\n",
      "Iteration 11, loss = 0.00036807\n",
      "Iteration 12, loss = 0.00029544\n",
      "Iteration 13, loss = 0.00025094\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00580018\n",
      "Iteration 2, loss = 0.00215540\n",
      "Iteration 3, loss = 0.00148870\n",
      "Iteration 4, loss = 0.00111731\n",
      "Iteration 5, loss = 0.00083821\n",
      "Iteration 6, loss = 0.00058666\n",
      "Iteration 7, loss = 0.00040040\n",
      "Iteration 8, loss = 0.00029610\n",
      "Iteration 9, loss = 0.00024144\n",
      "Iteration 10, loss = 0.00020807\n",
      "Iteration 11, loss = 0.00018866\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00314275\n",
      "Iteration 2, loss = 0.00140267\n",
      "Iteration 3, loss = 0.00089012\n",
      "Iteration 4, loss = 0.00048745\n",
      "Iteration 5, loss = 0.00026147\n",
      "Iteration 6, loss = 0.00017875\n",
      "Iteration 7, loss = 0.00015180\n",
      "Iteration 8, loss = 0.00013121\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20990802\n",
      "Iteration 2, loss = 0.04281926\n",
      "Iteration 3, loss = 0.02037767\n",
      "Iteration 4, loss = 0.01259577\n",
      "Iteration 5, loss = 0.00972792\n",
      "Iteration 6, loss = 0.00849198\n",
      "Iteration 7, loss = 0.00773559\n",
      "Iteration 8, loss = 0.00713443\n",
      "Iteration 9, loss = 0.00659678\n",
      "Iteration 10, loss = 0.00612599\n",
      "Iteration 11, loss = 0.00572272\n",
      "Iteration 12, loss = 0.00538330\n",
      "Iteration 13, loss = 0.00508168\n",
      "Iteration 14, loss = 0.00483486\n",
      "Iteration 15, loss = 0.00462478\n",
      "Iteration 16, loss = 0.00444605\n",
      "Iteration 17, loss = 0.00429815\n",
      "Iteration 18, loss = 0.00417762\n",
      "Iteration 19, loss = 0.00407734\n",
      "Iteration 20, loss = 0.00399253\n",
      "Iteration 21, loss = 0.00392517\n",
      "Iteration 22, loss = 0.00386561\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.08859842\n",
      "Iteration 2, loss = 0.01340905\n",
      "Iteration 3, loss = 0.00756334\n",
      "Iteration 4, loss = 0.00588225\n",
      "Iteration 5, loss = 0.00508818\n",
      "Iteration 6, loss = 0.00468037\n",
      "Iteration 7, loss = 0.00444852\n",
      "Iteration 8, loss = 0.00430049\n",
      "Iteration 9, loss = 0.00419852\n",
      "Iteration 10, loss = 0.00412216\n",
      "Iteration 11, loss = 0.00406165\n",
      "Iteration 12, loss = 0.00401129\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03138262\n",
      "Iteration 2, loss = 0.00737833\n",
      "Iteration 3, loss = 0.00559372\n",
      "Iteration 4, loss = 0.00468191\n",
      "Iteration 5, loss = 0.00399155\n",
      "Iteration 6, loss = 0.00344967\n",
      "Iteration 7, loss = 0.00303645\n",
      "Iteration 8, loss = 0.00271075\n",
      "Iteration 9, loss = 0.00244271\n",
      "Iteration 10, loss = 0.00222716\n",
      "Iteration 11, loss = 0.00203271\n",
      "Iteration 12, loss = 0.00186947\n",
      "Iteration 13, loss = 0.00172850\n",
      "Iteration 14, loss = 0.00159544\n",
      "Iteration 15, loss = 0.00147729\n",
      "Iteration 16, loss = 0.00137034\n",
      "Iteration 17, loss = 0.00126725\n",
      "Iteration 18, loss = 0.00117059\n",
      "Iteration 19, loss = 0.00107089\n",
      "Iteration 20, loss = 0.00097312\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01079628\n",
      "Iteration 2, loss = 0.00433655\n",
      "Iteration 3, loss = 0.00313174\n",
      "Iteration 4, loss = 0.00242954\n",
      "Iteration 5, loss = 0.00201693\n",
      "Iteration 6, loss = 0.00174094\n",
      "Iteration 7, loss = 0.00149585\n",
      "Iteration 8, loss = 0.00128878\n",
      "Iteration 9, loss = 0.00108451\n",
      "Iteration 10, loss = 0.00091805\n",
      "Iteration 11, loss = 0.00075056\n",
      "Iteration 12, loss = 0.00061096\n",
      "Iteration 13, loss = 0.00048697\n",
      "Iteration 14, loss = 0.00038934\n",
      "Iteration 15, loss = 0.00031811\n",
      "Iteration 16, loss = 0.00026876\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00636071\n",
      "Iteration 2, loss = 0.00293586\n",
      "Iteration 3, loss = 0.00204789\n",
      "Iteration 4, loss = 0.00164376\n",
      "Iteration 5, loss = 0.00137088\n",
      "Iteration 6, loss = 0.00113035\n",
      "Iteration 7, loss = 0.00090604\n",
      "Iteration 8, loss = 0.00070090\n",
      "Iteration 9, loss = 0.00052516\n",
      "Iteration 10, loss = 0.00040528\n",
      "Iteration 11, loss = 0.00032436\n",
      "Iteration 12, loss = 0.00027314\n",
      "Iteration 13, loss = 0.00024295\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00479912\n",
      "Iteration 2, loss = 0.00201668\n",
      "Iteration 3, loss = 0.00145837\n",
      "Iteration 4, loss = 0.00109240\n",
      "Iteration 5, loss = 0.00075684\n",
      "Iteration 6, loss = 0.00046631\n",
      "Iteration 7, loss = 0.00030166\n",
      "Iteration 8, loss = 0.00022933\n",
      "Iteration 9, loss = 0.00019439\n",
      "Iteration 10, loss = 0.00017133\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00324136\n",
      "Iteration 2, loss = 0.00139517\n",
      "Iteration 3, loss = 0.00087897\n",
      "Iteration 4, loss = 0.00047009\n",
      "Iteration 5, loss = 0.00025514\n",
      "Iteration 6, loss = 0.00017845\n",
      "Iteration 7, loss = 0.00015583\n",
      "Iteration 8, loss = 0.00013819\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32942222\n",
      "Iteration 2, loss = 0.07220890\n",
      "Iteration 3, loss = 0.02197978\n",
      "Iteration 4, loss = 0.01244973\n",
      "Iteration 5, loss = 0.00804662\n",
      "Iteration 6, loss = 0.00596185\n",
      "Iteration 7, loss = 0.00496567\n",
      "Iteration 8, loss = 0.00449034\n",
      "Iteration 9, loss = 0.00425990\n",
      "Iteration 10, loss = 0.00414495\n",
      "Iteration 11, loss = 0.00408072\n",
      "Iteration 12, loss = 0.00403860\n",
      "Iteration 13, loss = 0.00400735\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03990022\n",
      "Iteration 2, loss = 0.00977123\n",
      "Iteration 3, loss = 0.00607400\n",
      "Iteration 4, loss = 0.00500933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.00445690\n",
      "Iteration 6, loss = 0.00406952\n",
      "Iteration 7, loss = 0.00376802\n",
      "Iteration 8, loss = 0.00352184\n",
      "Iteration 9, loss = 0.00330855\n",
      "Iteration 10, loss = 0.00311812\n",
      "Iteration 11, loss = 0.00293832\n",
      "Iteration 12, loss = 0.00276905\n",
      "Iteration 13, loss = 0.00261443\n",
      "Iteration 14, loss = 0.00246455\n",
      "Iteration 15, loss = 0.00232899\n",
      "Iteration 16, loss = 0.00219930\n",
      "Iteration 17, loss = 0.00208183\n",
      "Iteration 18, loss = 0.00197235\n",
      "Iteration 19, loss = 0.00187820\n",
      "Iteration 20, loss = 0.00179429\n",
      "Iteration 21, loss = 0.00172583\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02434896\n",
      "Iteration 2, loss = 0.00682927\n",
      "Iteration 3, loss = 0.00491459\n",
      "Iteration 4, loss = 0.00386043\n",
      "Iteration 5, loss = 0.00318118\n",
      "Iteration 6, loss = 0.00273391\n",
      "Iteration 7, loss = 0.00241475\n",
      "Iteration 8, loss = 0.00217344\n",
      "Iteration 9, loss = 0.00198070\n",
      "Iteration 10, loss = 0.00184312\n",
      "Iteration 11, loss = 0.00171806\n",
      "Iteration 12, loss = 0.00161650\n",
      "Iteration 13, loss = 0.00152568\n",
      "Iteration 14, loss = 0.00143165\n",
      "Iteration 15, loss = 0.00134866\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01818579\n",
      "Iteration 2, loss = 0.00539578\n",
      "Iteration 3, loss = 0.00382132\n",
      "Iteration 4, loss = 0.00289149\n",
      "Iteration 5, loss = 0.00232461\n",
      "Iteration 6, loss = 0.00193915\n",
      "Iteration 7, loss = 0.00165171\n",
      "Iteration 8, loss = 0.00140869\n",
      "Iteration 9, loss = 0.00119188\n",
      "Iteration 10, loss = 0.00100823\n",
      "Iteration 11, loss = 0.00084275\n",
      "Iteration 12, loss = 0.00070689\n",
      "Iteration 13, loss = 0.00058356\n",
      "Iteration 14, loss = 0.00048540\n",
      "Iteration 15, loss = 0.00041022\n",
      "Iteration 16, loss = 0.00035338\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01190016\n",
      "Iteration 2, loss = 0.00408885\n",
      "Iteration 3, loss = 0.00280246\n",
      "Iteration 4, loss = 0.00212638\n",
      "Iteration 5, loss = 0.00174339\n",
      "Iteration 6, loss = 0.00148358\n",
      "Iteration 7, loss = 0.00126555\n",
      "Iteration 8, loss = 0.00109008\n",
      "Iteration 9, loss = 0.00092662\n",
      "Iteration 10, loss = 0.00078536\n",
      "Iteration 11, loss = 0.00065681\n",
      "Iteration 12, loss = 0.00054740\n",
      "Iteration 13, loss = 0.00044974\n",
      "Iteration 14, loss = 0.00037535\n",
      "Iteration 15, loss = 0.00031954\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00411121\n",
      "Iteration 2, loss = 0.00175643\n",
      "Iteration 3, loss = 0.00124433\n",
      "Iteration 4, loss = 0.00088517\n",
      "Iteration 5, loss = 0.00056963\n",
      "Iteration 6, loss = 0.00035410\n",
      "Iteration 7, loss = 0.00024744\n",
      "Iteration 8, loss = 0.00020693\n",
      "Iteration 9, loss = 0.00017870\n",
      "Iteration 10, loss = 0.00016002\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00328883\n",
      "Iteration 2, loss = 0.00141402\n",
      "Iteration 3, loss = 0.00093966\n",
      "Iteration 4, loss = 0.00053623\n",
      "Iteration 5, loss = 0.00028873\n",
      "Iteration 6, loss = 0.00019668\n",
      "Iteration 7, loss = 0.00016720\n",
      "Iteration 8, loss = 0.00014924\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30560443\n",
      "Iteration 2, loss = 0.04338810\n",
      "Iteration 3, loss = 0.01846002\n",
      "Iteration 4, loss = 0.01128287\n",
      "Iteration 5, loss = 0.00807507\n",
      "Iteration 6, loss = 0.00673829\n",
      "Iteration 7, loss = 0.00615612\n",
      "Iteration 8, loss = 0.00584630\n",
      "Iteration 9, loss = 0.00563150\n",
      "Iteration 10, loss = 0.00545561\n",
      "Iteration 11, loss = 0.00529259\n",
      "Iteration 12, loss = 0.00515028\n",
      "Iteration 13, loss = 0.00501798\n",
      "Iteration 14, loss = 0.00489952\n",
      "Iteration 15, loss = 0.00478592\n",
      "Iteration 16, loss = 0.00468554\n",
      "Iteration 17, loss = 0.00459778\n",
      "Iteration 18, loss = 0.00451651\n",
      "Iteration 19, loss = 0.00444069\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04324447\n",
      "Iteration 2, loss = 0.01293357\n",
      "Iteration 3, loss = 0.00796187\n",
      "Iteration 4, loss = 0.00607819\n",
      "Iteration 5, loss = 0.00492005\n",
      "Iteration 6, loss = 0.00411797\n",
      "Iteration 7, loss = 0.00352829\n",
      "Iteration 8, loss = 0.00308013\n",
      "Iteration 9, loss = 0.00274427\n",
      "Iteration 10, loss = 0.00250621\n",
      "Iteration 11, loss = 0.00234135\n",
      "Iteration 12, loss = 0.00221497\n",
      "Iteration 13, loss = 0.00211531\n",
      "Iteration 14, loss = 0.00203953\n",
      "Iteration 15, loss = 0.00195365\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01584892\n",
      "Iteration 2, loss = 0.00543903\n",
      "Iteration 3, loss = 0.00431693\n",
      "Iteration 4, loss = 0.00364968\n",
      "Iteration 5, loss = 0.00309811\n",
      "Iteration 6, loss = 0.00264262\n",
      "Iteration 7, loss = 0.00228636\n",
      "Iteration 8, loss = 0.00201939\n",
      "Iteration 9, loss = 0.00177822\n",
      "Iteration 10, loss = 0.00159520\n",
      "Iteration 11, loss = 0.00144375\n",
      "Iteration 12, loss = 0.00128794\n",
      "Iteration 13, loss = 0.00111927\n",
      "Iteration 14, loss = 0.00095556\n",
      "Iteration 15, loss = 0.00081335\n",
      "Iteration 16, loss = 0.00068426\n",
      "Iteration 17, loss = 0.00057838\n",
      "Iteration 18, loss = 0.00049555\n",
      "Iteration 19, loss = 0.00042898\n",
      "Iteration 20, loss = 0.00037787\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01091590\n",
      "Iteration 2, loss = 0.00459282\n",
      "Iteration 3, loss = 0.00313850\n",
      "Iteration 4, loss = 0.00234180\n",
      "Iteration 5, loss = 0.00189961\n",
      "Iteration 6, loss = 0.00158738\n",
      "Iteration 7, loss = 0.00134155\n",
      "Iteration 8, loss = 0.00113605\n",
      "Iteration 9, loss = 0.00094415\n",
      "Iteration 10, loss = 0.00078212\n",
      "Iteration 11, loss = 0.00064428\n",
      "Iteration 12, loss = 0.00052203\n",
      "Iteration 13, loss = 0.00043306\n",
      "Iteration 14, loss = 0.00036678\n",
      "Iteration 15, loss = 0.00031473\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01341040\n",
      "Iteration 2, loss = 0.00420816\n",
      "Iteration 3, loss = 0.00271261\n",
      "Iteration 4, loss = 0.00190313\n",
      "Iteration 5, loss = 0.00142056\n",
      "Iteration 6, loss = 0.00107504\n",
      "Iteration 7, loss = 0.00082441\n",
      "Iteration 8, loss = 0.00064457\n",
      "Iteration 9, loss = 0.00052095\n",
      "Iteration 10, loss = 0.00043278\n",
      "Iteration 11, loss = 0.00037576\n",
      "Iteration 12, loss = 0.00033288\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00557954\n",
      "Iteration 2, loss = 0.00232414\n",
      "Iteration 3, loss = 0.00166273\n",
      "Iteration 4, loss = 0.00128994\n",
      "Iteration 5, loss = 0.00095095\n",
      "Iteration 6, loss = 0.00065240\n",
      "Iteration 7, loss = 0.00042454\n",
      "Iteration 8, loss = 0.00030028\n",
      "Iteration 9, loss = 0.00024488\n",
      "Iteration 10, loss = 0.00021212\n",
      "Iteration 11, loss = 0.00018925\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00328739\n",
      "Iteration 2, loss = 0.00144292\n",
      "Iteration 3, loss = 0.00094720\n",
      "Iteration 4, loss = 0.00050562\n",
      "Iteration 5, loss = 0.00025572\n",
      "Iteration 6, loss = 0.00018276\n",
      "Iteration 7, loss = 0.00016128\n",
      "Iteration 8, loss = 0.00014273\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06372775\n",
      "Iteration 2, loss = 0.01853821\n",
      "Iteration 3, loss = 0.00909668\n",
      "Iteration 4, loss = 0.00621719\n",
      "Iteration 5, loss = 0.00501982\n",
      "Iteration 6, loss = 0.00448180\n",
      "Iteration 7, loss = 0.00421439\n",
      "Iteration 8, loss = 0.00405815\n",
      "Iteration 9, loss = 0.00396038\n",
      "Iteration 10, loss = 0.00389123\n",
      "Iteration 11, loss = 0.00384135\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.11626242\n",
      "Iteration 2, loss = 0.02820756\n",
      "Iteration 3, loss = 0.01410260\n",
      "Iteration 4, loss = 0.00990380\n",
      "Iteration 5, loss = 0.00817321\n",
      "Iteration 6, loss = 0.00709256\n",
      "Iteration 7, loss = 0.00623133\n",
      "Iteration 8, loss = 0.00550511\n",
      "Iteration 9, loss = 0.00489304\n",
      "Iteration 10, loss = 0.00439574\n",
      "Iteration 11, loss = 0.00400831\n",
      "Iteration 12, loss = 0.00367234\n",
      "Iteration 13, loss = 0.00338072\n",
      "Iteration 14, loss = 0.00311258\n",
      "Iteration 15, loss = 0.00288867\n",
      "Iteration 16, loss = 0.00267302\n",
      "Iteration 17, loss = 0.00248340\n",
      "Iteration 18, loss = 0.00231767\n",
      "Iteration 19, loss = 0.00217924\n",
      "Iteration 20, loss = 0.00206074\n",
      "Iteration 21, loss = 0.00195982\n",
      "Iteration 22, loss = 0.00188455\n",
      "Iteration 23, loss = 0.00180765\n",
      "Iteration 24, loss = 0.00174513\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01867501\n",
      "Iteration 2, loss = 0.00660589\n",
      "Iteration 3, loss = 0.00492681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.00389750\n",
      "Iteration 5, loss = 0.00317545\n",
      "Iteration 6, loss = 0.00268115\n",
      "Iteration 7, loss = 0.00233361\n",
      "Iteration 8, loss = 0.00208286\n",
      "Iteration 9, loss = 0.00187787\n",
      "Iteration 10, loss = 0.00171289\n",
      "Iteration 11, loss = 0.00157411\n",
      "Iteration 12, loss = 0.00144332\n",
      "Iteration 13, loss = 0.00131815\n",
      "Iteration 14, loss = 0.00118622\n",
      "Iteration 15, loss = 0.00105916\n",
      "Iteration 16, loss = 0.00093477\n",
      "Iteration 17, loss = 0.00083005\n",
      "Iteration 18, loss = 0.00073245\n",
      "Iteration 19, loss = 0.00065001\n",
      "Iteration 20, loss = 0.00057395\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01048737\n",
      "Iteration 2, loss = 0.00430164\n",
      "Iteration 3, loss = 0.00306917\n",
      "Iteration 4, loss = 0.00232914\n",
      "Iteration 5, loss = 0.00185246\n",
      "Iteration 6, loss = 0.00150082\n",
      "Iteration 7, loss = 0.00122692\n",
      "Iteration 8, loss = 0.00101092\n",
      "Iteration 9, loss = 0.00082651\n",
      "Iteration 10, loss = 0.00067824\n",
      "Iteration 11, loss = 0.00055810\n",
      "Iteration 12, loss = 0.00046828\n",
      "Iteration 13, loss = 0.00039850\n",
      "Iteration 14, loss = 0.00034220\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00631443\n",
      "Iteration 2, loss = 0.00297812\n",
      "Iteration 3, loss = 0.00205688\n",
      "Iteration 4, loss = 0.00159859\n",
      "Iteration 5, loss = 0.00127807\n",
      "Iteration 6, loss = 0.00098262\n",
      "Iteration 7, loss = 0.00073120\n",
      "Iteration 8, loss = 0.00053104\n",
      "Iteration 9, loss = 0.00038733\n",
      "Iteration 10, loss = 0.00031188\n",
      "Iteration 11, loss = 0.00026001\n",
      "Iteration 12, loss = 0.00023441\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00435042\n",
      "Iteration 2, loss = 0.00181737\n",
      "Iteration 3, loss = 0.00127038\n",
      "Iteration 4, loss = 0.00088405\n",
      "Iteration 5, loss = 0.00057791\n",
      "Iteration 6, loss = 0.00037626\n",
      "Iteration 7, loss = 0.00027319\n",
      "Iteration 8, loss = 0.00021687\n",
      "Iteration 9, loss = 0.00018887\n",
      "Iteration 10, loss = 0.00016880\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00381416\n",
      "Iteration 2, loss = 0.00145559\n",
      "Iteration 3, loss = 0.00098000\n",
      "Iteration 4, loss = 0.00052761\n",
      "Iteration 5, loss = 0.00028444\n",
      "Iteration 6, loss = 0.00019857\n",
      "Iteration 7, loss = 0.00016835\n",
      "Iteration 8, loss = 0.00014989\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.13730580\n",
      "Iteration 2, loss = 0.03881908\n",
      "Iteration 3, loss = 0.01588697\n",
      "Iteration 4, loss = 0.00889759\n",
      "Iteration 5, loss = 0.00679877\n",
      "Iteration 6, loss = 0.00596922\n",
      "Iteration 7, loss = 0.00548969\n",
      "Iteration 8, loss = 0.00513846\n",
      "Iteration 9, loss = 0.00485188\n",
      "Iteration 10, loss = 0.00461918\n",
      "Iteration 11, loss = 0.00442670\n",
      "Iteration 12, loss = 0.00426971\n",
      "Iteration 13, loss = 0.00414193\n",
      "Iteration 14, loss = 0.00402772\n",
      "Iteration 15, loss = 0.00393003\n",
      "Iteration 16, loss = 0.00383298\n",
      "Iteration 17, loss = 0.00372630\n",
      "Iteration 18, loss = 0.00361148\n",
      "Iteration 19, loss = 0.00349922\n",
      "Iteration 20, loss = 0.00338070\n",
      "Iteration 21, loss = 0.00324313\n",
      "Iteration 22, loss = 0.00308925\n",
      "Iteration 23, loss = 0.00292217\n",
      "Iteration 24, loss = 0.00272562\n",
      "Iteration 25, loss = 0.00254912\n",
      "Iteration 26, loss = 0.00242466\n",
      "Iteration 27, loss = 0.00233734\n",
      "Iteration 28, loss = 0.00227108\n",
      "Iteration 29, loss = 0.00220537\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17456891\n",
      "Iteration 2, loss = 0.02072980\n",
      "Iteration 3, loss = 0.00978651\n",
      "Iteration 4, loss = 0.00674622\n",
      "Iteration 5, loss = 0.00576671\n",
      "Iteration 6, loss = 0.00529495\n",
      "Iteration 7, loss = 0.00493658\n",
      "Iteration 8, loss = 0.00458815\n",
      "Iteration 9, loss = 0.00424333\n",
      "Iteration 10, loss = 0.00390734\n",
      "Iteration 11, loss = 0.00359556\n",
      "Iteration 12, loss = 0.00331199\n",
      "Iteration 13, loss = 0.00306676\n",
      "Iteration 14, loss = 0.00285239\n",
      "Iteration 15, loss = 0.00266931\n",
      "Iteration 16, loss = 0.00251513\n",
      "Iteration 17, loss = 0.00237458\n",
      "Iteration 18, loss = 0.00225499\n",
      "Iteration 19, loss = 0.00214620\n",
      "Iteration 20, loss = 0.00205402\n",
      "Iteration 21, loss = 0.00196517\n",
      "Iteration 22, loss = 0.00188945\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.09461600\n",
      "Iteration 2, loss = 0.01519905\n",
      "Iteration 3, loss = 0.00806753\n",
      "Iteration 4, loss = 0.00601628\n",
      "Iteration 5, loss = 0.00528745\n",
      "Iteration 6, loss = 0.00475592\n",
      "Iteration 7, loss = 0.00418547\n",
      "Iteration 8, loss = 0.00362426\n",
      "Iteration 9, loss = 0.00314991\n",
      "Iteration 10, loss = 0.00279886\n",
      "Iteration 11, loss = 0.00255949\n",
      "Iteration 12, loss = 0.00237854\n",
      "Iteration 13, loss = 0.00224283\n",
      "Iteration 14, loss = 0.00212167\n",
      "Iteration 15, loss = 0.00200325\n",
      "Iteration 16, loss = 0.00189046\n",
      "Iteration 17, loss = 0.00177967\n",
      "Iteration 18, loss = 0.00166997\n",
      "Iteration 19, loss = 0.00155703\n",
      "Iteration 20, loss = 0.00144825\n",
      "Iteration 21, loss = 0.00134477\n",
      "Iteration 22, loss = 0.00125036\n",
      "Iteration 23, loss = 0.00115664\n",
      "Iteration 24, loss = 0.00106896\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01504511\n",
      "Iteration 2, loss = 0.00480497\n",
      "Iteration 3, loss = 0.00343419\n",
      "Iteration 4, loss = 0.00264013\n",
      "Iteration 5, loss = 0.00218629\n",
      "Iteration 6, loss = 0.00189247\n",
      "Iteration 7, loss = 0.00168176\n",
      "Iteration 8, loss = 0.00150164\n",
      "Iteration 9, loss = 0.00135220\n",
      "Iteration 10, loss = 0.00120639\n",
      "Iteration 11, loss = 0.00107033\n",
      "Iteration 12, loss = 0.00094908\n",
      "Iteration 13, loss = 0.00083413\n",
      "Iteration 14, loss = 0.00072822\n",
      "Iteration 15, loss = 0.00063402\n",
      "Iteration 16, loss = 0.00054356\n",
      "Iteration 17, loss = 0.00045901\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01099165\n",
      "Iteration 2, loss = 0.00373097\n",
      "Iteration 3, loss = 0.00244664\n",
      "Iteration 4, loss = 0.00190346\n",
      "Iteration 5, loss = 0.00156099\n",
      "Iteration 6, loss = 0.00131817\n",
      "Iteration 7, loss = 0.00109083\n",
      "Iteration 8, loss = 0.00087277\n",
      "Iteration 9, loss = 0.00070120\n",
      "Iteration 10, loss = 0.00055269\n",
      "Iteration 11, loss = 0.00044450\n",
      "Iteration 12, loss = 0.00036969\n",
      "Iteration 13, loss = 0.00031889\n",
      "Iteration 14, loss = 0.00028245\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00555765\n",
      "Iteration 2, loss = 0.00216439\n",
      "Iteration 3, loss = 0.00153597\n",
      "Iteration 4, loss = 0.00114846\n",
      "Iteration 5, loss = 0.00084232\n",
      "Iteration 6, loss = 0.00058358\n",
      "Iteration 7, loss = 0.00039740\n",
      "Iteration 8, loss = 0.00029377\n",
      "Iteration 9, loss = 0.00023682\n",
      "Iteration 10, loss = 0.00020755\n",
      "Iteration 11, loss = 0.00018583\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00376058\n",
      "Iteration 2, loss = 0.00155821\n",
      "Iteration 3, loss = 0.00106145\n",
      "Iteration 4, loss = 0.00063771\n",
      "Iteration 5, loss = 0.00032800\n",
      "Iteration 6, loss = 0.00020753\n",
      "Iteration 7, loss = 0.00017043\n",
      "Iteration 8, loss = 0.00015296\n",
      "Iteration 9, loss = 0.00013358\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.09751058\n",
      "Iteration 2, loss = 0.01846834\n",
      "Iteration 3, loss = 0.00612032\n",
      "Iteration 4, loss = 0.00509663\n",
      "Iteration 5, loss = 0.00478333\n",
      "Iteration 6, loss = 0.00453263\n",
      "Iteration 7, loss = 0.00433547\n",
      "Iteration 8, loss = 0.00418720\n",
      "Iteration 9, loss = 0.00407909\n",
      "Iteration 10, loss = 0.00400302\n",
      "Iteration 11, loss = 0.00395080\n",
      "Iteration 12, loss = 0.00391437\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00619145\n",
      "Iteration 2, loss = 0.00404927\n",
      "Iteration 3, loss = 0.00388381\n",
      "Iteration 4, loss = 0.00386853\n",
      "Iteration 5, loss = 0.00386625\n",
      "Iteration 6, loss = 0.00386801\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.09157502\n",
      "Iteration 2, loss = 0.00479907\n",
      "Iteration 3, loss = 0.00434254\n",
      "Iteration 4, loss = 0.00408861\n",
      "Iteration 5, loss = 0.00395940\n",
      "Iteration 6, loss = 0.00389967\n",
      "Iteration 7, loss = 0.00387551\n",
      "Iteration 8, loss = 0.00386373\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00567799\n",
      "Iteration 2, loss = 0.00387751\n",
      "Iteration 3, loss = 0.00387733\n",
      "Iteration 4, loss = 0.00389236\n",
      "Iteration 5, loss = 0.00389986\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.00415825\n",
      "Iteration 2, loss = 0.00391566\n",
      "Iteration 3, loss = 0.00395573\n",
      "Iteration 4, loss = 0.00395077\n",
      "Iteration 5, loss = 0.00396950\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00435556\n",
      "Iteration 2, loss = 0.00395890\n",
      "Iteration 3, loss = 0.00395389\n",
      "Iteration 4, loss = 0.00394952\n",
      "Iteration 5, loss = 0.00404542\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01008284\n",
      "Iteration 2, loss = 0.00390634\n",
      "Iteration 3, loss = 0.00390140\n",
      "Iteration 4, loss = 0.00392589\n",
      "Iteration 5, loss = 0.00394261\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00512150\n",
      "Iteration 2, loss = 0.00430557\n",
      "Iteration 3, loss = 0.00402725\n",
      "Iteration 4, loss = 0.00397104\n",
      "Iteration 5, loss = 0.00396195\n",
      "Iteration 6, loss = 0.00395442\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00494281\n",
      "Iteration 2, loss = 0.00402404\n",
      "Iteration 3, loss = 0.00397775\n",
      "Iteration 4, loss = 0.00397198\n",
      "Iteration 5, loss = 0.00397536\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00800444\n",
      "Iteration 2, loss = 0.00436008\n",
      "Iteration 3, loss = 0.00402931\n",
      "Iteration 4, loss = 0.00397109\n",
      "Iteration 5, loss = 0.00396194\n",
      "Iteration 6, loss = 0.00396161\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00602310\n",
      "Iteration 2, loss = 0.00399178\n",
      "Iteration 3, loss = 0.00398247\n",
      "Iteration 4, loss = 0.00398731\n",
      "Iteration 5, loss = 0.00398877\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00999490\n",
      "Iteration 2, loss = 0.00400428\n",
      "Iteration 3, loss = 0.00398290\n",
      "Iteration 4, loss = 0.00398502\n",
      "Iteration 5, loss = 0.00398565\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00429588\n",
      "Iteration 2, loss = 0.00403366\n",
      "Iteration 3, loss = 0.00404182\n",
      "Iteration 4, loss = 0.00409273\n",
      "Iteration 5, loss = 0.00409582\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00471267\n",
      "Iteration 2, loss = 0.00410498\n",
      "Iteration 3, loss = 0.00416713\n",
      "Iteration 4, loss = 0.00417877\n",
      "Iteration 5, loss = 0.00416880\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25242600\n",
      "Iteration 2, loss = 0.06201547\n",
      "Iteration 3, loss = 0.01417230\n",
      "Iteration 4, loss = 0.00619686\n",
      "Iteration 5, loss = 0.00513070\n",
      "Iteration 6, loss = 0.00481310\n",
      "Iteration 7, loss = 0.00457943\n",
      "Iteration 8, loss = 0.00439188\n",
      "Iteration 9, loss = 0.00424677\n",
      "Iteration 10, loss = 0.00413611\n",
      "Iteration 11, loss = 0.00405404\n",
      "Iteration 12, loss = 0.00399575\n",
      "Iteration 13, loss = 0.00395272\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.09993267\n",
      "Iteration 2, loss = 0.00765871\n",
      "Iteration 3, loss = 0.00504206\n",
      "Iteration 4, loss = 0.00451535\n",
      "Iteration 5, loss = 0.00419715\n",
      "Iteration 6, loss = 0.00402360\n",
      "Iteration 7, loss = 0.00393599\n",
      "Iteration 8, loss = 0.00389211\n",
      "Iteration 9, loss = 0.00386928\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00457026\n",
      "Iteration 2, loss = 0.00389136\n",
      "Iteration 3, loss = 0.00388090\n",
      "Iteration 4, loss = 0.00388232\n",
      "Iteration 5, loss = 0.00388711\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00685445\n",
      "Iteration 2, loss = 0.00391143\n",
      "Iteration 3, loss = 0.00387798\n",
      "Iteration 4, loss = 0.00388279\n",
      "Iteration 5, loss = 0.00388698\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00590447\n",
      "Iteration 2, loss = 0.00390799\n",
      "Iteration 3, loss = 0.00389916\n",
      "Iteration 4, loss = 0.00389384\n",
      "Iteration 5, loss = 0.00392885\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00561973\n",
      "Iteration 2, loss = 0.00395273\n",
      "Iteration 3, loss = 0.00391852\n",
      "Iteration 4, loss = 0.00394062\n",
      "Iteration 5, loss = 0.00395747\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01194960\n",
      "Iteration 2, loss = 0.00389123\n",
      "Iteration 3, loss = 0.00391496\n",
      "Iteration 4, loss = 0.00392224\n",
      "Iteration 5, loss = 0.00391600\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.16941646\n",
      "Iteration 2, loss = 0.04073061\n",
      "Iteration 3, loss = 0.01034407\n",
      "Iteration 4, loss = 0.00589899\n",
      "Iteration 5, loss = 0.00529738\n",
      "Iteration 6, loss = 0.00501159\n",
      "Iteration 7, loss = 0.00477527\n",
      "Iteration 8, loss = 0.00457886\n",
      "Iteration 9, loss = 0.00441867\n",
      "Iteration 10, loss = 0.00429307\n",
      "Iteration 11, loss = 0.00419440\n",
      "Iteration 12, loss = 0.00412057\n",
      "Iteration 13, loss = 0.00406398\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01761192\n",
      "Iteration 2, loss = 0.00517942\n",
      "Iteration 3, loss = 0.00442299\n",
      "Iteration 4, loss = 0.00411602\n",
      "Iteration 5, loss = 0.00400878\n",
      "Iteration 6, loss = 0.00397206\n",
      "Iteration 7, loss = 0.00396114\n",
      "Iteration 8, loss = 0.00395824\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00469731\n",
      "Iteration 2, loss = 0.00398940\n",
      "Iteration 3, loss = 0.00397977\n",
      "Iteration 4, loss = 0.00398086\n",
      "Iteration 5, loss = 0.00398237\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01375886\n",
      "Iteration 2, loss = 0.00409258\n",
      "Iteration 3, loss = 0.00397904\n",
      "Iteration 4, loss = 0.00397218\n",
      "Iteration 5, loss = 0.00397229\n",
      "Iteration 6, loss = 0.00397431\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00572220\n",
      "Iteration 2, loss = 0.00398941\n",
      "Iteration 3, loss = 0.00398553\n",
      "Iteration 4, loss = 0.00400370\n",
      "Iteration 5, loss = 0.00400345\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00430638\n",
      "Iteration 2, loss = 0.00405123\n",
      "Iteration 3, loss = 0.00404616\n",
      "Iteration 4, loss = 0.00406377\n",
      "Iteration 5, loss = 0.00406721\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01037836\n",
      "Iteration 2, loss = 0.00401028\n",
      "Iteration 3, loss = 0.00403940\n",
      "Iteration 4, loss = 0.00406360\n",
      "Iteration 5, loss = 0.00404415\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.09536122\n",
      "Iteration 2, loss = 0.01454347\n",
      "Iteration 3, loss = 0.00538425\n",
      "Iteration 4, loss = 0.00476714\n",
      "Iteration 5, loss = 0.00451965\n",
      "Iteration 6, loss = 0.00432812\n",
      "Iteration 7, loss = 0.00418686\n",
      "Iteration 8, loss = 0.00408656\n",
      "Iteration 9, loss = 0.00401514\n",
      "Iteration 10, loss = 0.00396670\n",
      "Iteration 11, loss = 0.00393367\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00797585\n",
      "Iteration 2, loss = 0.00436829\n",
      "Iteration 3, loss = 0.00398120\n",
      "Iteration 4, loss = 0.00388490\n",
      "Iteration 5, loss = 0.00386528\n",
      "Iteration 6, loss = 0.00386469\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.14291557\n",
      "Iteration 2, loss = 0.00593782\n",
      "Iteration 3, loss = 0.00485980\n",
      "Iteration 4, loss = 0.00441904\n",
      "Iteration 5, loss = 0.00415402\n",
      "Iteration 6, loss = 0.00400833\n",
      "Iteration 7, loss = 0.00393197\n",
      "Iteration 8, loss = 0.00389290\n",
      "Iteration 9, loss = 0.00387452\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00541951\n",
      "Iteration 2, loss = 0.00388686\n",
      "Iteration 3, loss = 0.00390248\n",
      "Iteration 4, loss = 0.00390314\n",
      "Iteration 5, loss = 0.00391640\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00421998\n",
      "Iteration 2, loss = 0.00394053\n",
      "Iteration 3, loss = 0.00401471\n",
      "Iteration 4, loss = 0.00393349\n",
      "Iteration 5, loss = 0.00392952\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.00430296\n",
      "Iteration 2, loss = 0.00394356\n",
      "Iteration 3, loss = 0.00394443\n",
      "Iteration 4, loss = 0.00400134\n",
      "Iteration 5, loss = 0.00401918\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01991351\n",
      "Iteration 2, loss = 0.00390243\n",
      "Iteration 3, loss = 0.00389164\n",
      "Iteration 4, loss = 0.00390112\n",
      "Iteration 5, loss = 0.00391468\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.09606506\n",
      "Iteration 2, loss = 0.01228916\n",
      "Iteration 3, loss = 0.00514464\n",
      "Iteration 4, loss = 0.00462769\n",
      "Iteration 5, loss = 0.00437217\n",
      "Iteration 6, loss = 0.00420407\n",
      "Iteration 7, loss = 0.00409730\n",
      "Iteration 8, loss = 0.00403111\n",
      "Iteration 9, loss = 0.00398981\n",
      "Iteration 10, loss = 0.00396755\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00931408\n",
      "Iteration 2, loss = 0.00432398\n",
      "Iteration 3, loss = 0.00402375\n",
      "Iteration 4, loss = 0.00397162\n",
      "Iteration 5, loss = 0.00396719\n",
      "Iteration 6, loss = 0.00396140\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00506966\n",
      "Iteration 2, loss = 0.00400223\n",
      "Iteration 3, loss = 0.00396890\n",
      "Iteration 4, loss = 0.00397158\n",
      "Iteration 5, loss = 0.00397271\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00645780\n",
      "Iteration 2, loss = 0.00399475\n",
      "Iteration 3, loss = 0.00397416\n",
      "Iteration 4, loss = 0.00397017\n",
      "Iteration 5, loss = 0.00399426\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00427133\n",
      "Iteration 2, loss = 0.00405189\n",
      "Iteration 3, loss = 0.00402414\n",
      "Iteration 4, loss = 0.00403907\n",
      "Iteration 5, loss = 0.00408359\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00900116\n",
      "Iteration 2, loss = 0.00398381\n",
      "Iteration 3, loss = 0.00400208\n",
      "Iteration 4, loss = 0.00400644\n",
      "Iteration 5, loss = 0.00402180\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00850580\n",
      "Iteration 2, loss = 0.00401738\n",
      "Iteration 3, loss = 0.00404936\n",
      "Iteration 4, loss = 0.00403161\n",
      "Iteration 5, loss = 0.00403000\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04219174\n",
      "Iteration 2, loss = 0.00772034\n",
      "Iteration 3, loss = 0.00575956\n",
      "Iteration 4, loss = 0.00502703\n",
      "Iteration 5, loss = 0.00453779\n",
      "Iteration 6, loss = 0.00423339\n",
      "Iteration 7, loss = 0.00405570\n",
      "Iteration 8, loss = 0.00395606\n",
      "Iteration 9, loss = 0.00390093\n",
      "Iteration 10, loss = 0.00387032\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.07320309\n",
      "Iteration 2, loss = 0.00580768\n",
      "Iteration 3, loss = 0.00469932\n",
      "Iteration 4, loss = 0.00433655\n",
      "Iteration 5, loss = 0.00411207\n",
      "Iteration 6, loss = 0.00398664\n",
      "Iteration 7, loss = 0.00391790\n",
      "Iteration 8, loss = 0.00388410\n",
      "Iteration 9, loss = 0.00386636\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00916306\n",
      "Iteration 2, loss = 0.00416566\n",
      "Iteration 3, loss = 0.00390568\n",
      "Iteration 4, loss = 0.00386853\n",
      "Iteration 5, loss = 0.00386830\n",
      "Iteration 6, loss = 0.00387017\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00630004\n",
      "Iteration 2, loss = 0.00388864\n",
      "Iteration 3, loss = 0.00387708\n",
      "Iteration 4, loss = 0.00388912\n",
      "Iteration 5, loss = 0.00387453\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04156969\n",
      "Iteration 2, loss = 0.00418736\n",
      "Iteration 3, loss = 0.00390808\n",
      "Iteration 4, loss = 0.00387143\n",
      "Iteration 5, loss = 0.00386437\n",
      "Iteration 6, loss = 0.00387098\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00432362\n",
      "Iteration 2, loss = 0.00394942\n",
      "Iteration 3, loss = 0.00393723\n",
      "Iteration 4, loss = 0.00408557\n",
      "Iteration 5, loss = 0.00396738\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00773487\n",
      "Iteration 2, loss = 0.00391301\n",
      "Iteration 3, loss = 0.00396043\n",
      "Iteration 4, loss = 0.00395996\n",
      "Iteration 5, loss = 0.00398192\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00519300\n",
      "Iteration 2, loss = 0.00425072\n",
      "Iteration 3, loss = 0.00402167\n",
      "Iteration 4, loss = 0.00397626\n",
      "Iteration 5, loss = 0.00396511\n",
      "Iteration 6, loss = 0.00396456\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.10403888\n",
      "Iteration 2, loss = 0.00875867\n",
      "Iteration 3, loss = 0.00533181\n",
      "Iteration 4, loss = 0.00485542\n",
      "Iteration 5, loss = 0.00451928\n",
      "Iteration 6, loss = 0.00430027\n",
      "Iteration 7, loss = 0.00416386\n",
      "Iteration 8, loss = 0.00408304\n",
      "Iteration 9, loss = 0.00403639\n",
      "Iteration 10, loss = 0.00400519\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23350256\n",
      "Iteration 2, loss = 0.00915491\n",
      "Iteration 3, loss = 0.00517414\n",
      "Iteration 4, loss = 0.00466151\n",
      "Iteration 5, loss = 0.00433632\n",
      "Iteration 6, loss = 0.00415201\n",
      "Iteration 7, loss = 0.00405208\n",
      "Iteration 8, loss = 0.00400105\n",
      "Iteration 9, loss = 0.00397522\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00505624\n",
      "Iteration 2, loss = 0.00398669\n",
      "Iteration 3, loss = 0.00398437\n",
      "Iteration 4, loss = 0.00399354\n",
      "Iteration 5, loss = 0.00399372\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00453878\n",
      "Iteration 2, loss = 0.00402062\n",
      "Iteration 3, loss = 0.00402483\n",
      "Iteration 4, loss = 0.00400686\n",
      "Iteration 5, loss = 0.00404827\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00442986\n",
      "Iteration 2, loss = 0.00406501\n",
      "Iteration 3, loss = 0.00410112\n",
      "Iteration 4, loss = 0.00406195\n",
      "Iteration 5, loss = 0.00404956\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00650143\n",
      "Iteration 2, loss = 0.00402959\n",
      "Iteration 3, loss = 0.00403748\n",
      "Iteration 4, loss = 0.00403641\n",
      "Iteration 5, loss = 0.00407573\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.13587414\n",
      "Iteration 2, loss = 0.02654414\n",
      "Iteration 3, loss = 0.00772965\n",
      "Iteration 4, loss = 0.00566070\n",
      "Iteration 5, loss = 0.00517412\n",
      "Iteration 6, loss = 0.00481749\n",
      "Iteration 7, loss = 0.00453841\n",
      "Iteration 8, loss = 0.00432880\n",
      "Iteration 9, loss = 0.00417648\n",
      "Iteration 10, loss = 0.00406952\n",
      "Iteration 11, loss = 0.00399629\n",
      "Iteration 12, loss = 0.00394688\n",
      "Iteration 13, loss = 0.00391418\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00566605\n",
      "Iteration 2, loss = 0.00408129\n",
      "Iteration 3, loss = 0.00388596\n",
      "Iteration 4, loss = 0.00386413\n",
      "Iteration 5, loss = 0.00386126\n",
      "Iteration 6, loss = 0.00386060\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04266662\n",
      "Iteration 2, loss = 0.00491209\n",
      "Iteration 3, loss = 0.00430430\n",
      "Iteration 4, loss = 0.00403665\n",
      "Iteration 5, loss = 0.00393173\n",
      "Iteration 6, loss = 0.00389250\n",
      "Iteration 7, loss = 0.00387508\n",
      "Iteration 8, loss = 0.00387135\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00512893\n",
      "Iteration 2, loss = 0.00388415\n",
      "Iteration 3, loss = 0.00390025\n",
      "Iteration 4, loss = 0.00389165\n",
      "Iteration 5, loss = 0.00389781\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00558304\n",
      "Iteration 2, loss = 0.00389120\n",
      "Iteration 3, loss = 0.00389273\n",
      "Iteration 4, loss = 0.00390182\n",
      "Iteration 5, loss = 0.00390015\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00477129\n",
      "Iteration 2, loss = 0.00391656\n",
      "Iteration 3, loss = 0.00392179\n",
      "Iteration 4, loss = 0.00395427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.00395917\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00587711\n",
      "Iteration 2, loss = 0.00391757\n",
      "Iteration 3, loss = 0.00394436\n",
      "Iteration 4, loss = 0.00398778\n",
      "Iteration 5, loss = 0.00405033\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23892824\n",
      "Iteration 2, loss = 0.08043438\n",
      "Iteration 3, loss = 0.02644826\n",
      "Iteration 4, loss = 0.01026581\n",
      "Iteration 5, loss = 0.00624770\n",
      "Iteration 6, loss = 0.00539723\n",
      "Iteration 7, loss = 0.00517518\n",
      "Iteration 8, loss = 0.00504652\n",
      "Iteration 9, loss = 0.00493009\n",
      "Iteration 10, loss = 0.00481724\n",
      "Iteration 11, loss = 0.00470903\n",
      "Iteration 12, loss = 0.00460677\n",
      "Iteration 13, loss = 0.00451192\n",
      "Iteration 14, loss = 0.00442634\n",
      "Iteration 15, loss = 0.00434945\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01603787\n",
      "Iteration 2, loss = 0.00490160\n",
      "Iteration 3, loss = 0.00423519\n",
      "Iteration 4, loss = 0.00403477\n",
      "Iteration 5, loss = 0.00397741\n",
      "Iteration 6, loss = 0.00395962\n",
      "Iteration 7, loss = 0.00395691\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03119085\n",
      "Iteration 2, loss = 0.00458168\n",
      "Iteration 3, loss = 0.00416634\n",
      "Iteration 4, loss = 0.00402004\n",
      "Iteration 5, loss = 0.00397367\n",
      "Iteration 6, loss = 0.00396145\n",
      "Iteration 7, loss = 0.00395683\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00469284\n",
      "Iteration 2, loss = 0.00398895\n",
      "Iteration 3, loss = 0.00399152\n",
      "Iteration 4, loss = 0.00401195\n",
      "Iteration 5, loss = 0.00399256\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00701226\n",
      "Iteration 2, loss = 0.00399000\n",
      "Iteration 3, loss = 0.00399015\n",
      "Iteration 4, loss = 0.00400265\n",
      "Iteration 5, loss = 0.00399289\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00609929\n",
      "Iteration 2, loss = 0.00399342\n",
      "Iteration 3, loss = 0.00403052\n",
      "Iteration 4, loss = 0.00404757\n",
      "Iteration 5, loss = 0.00404817\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00535958\n",
      "Iteration 2, loss = 0.00403622\n",
      "Iteration 3, loss = 0.00407144\n",
      "Iteration 4, loss = 0.00407597\n",
      "Iteration 5, loss = 0.00408374\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06409494\n",
      "Iteration 2, loss = 0.01666947\n",
      "Iteration 3, loss = 0.00702865\n",
      "Iteration 4, loss = 0.00473773\n",
      "Iteration 5, loss = 0.00422054\n",
      "Iteration 6, loss = 0.00409473\n",
      "Iteration 7, loss = 0.00404205\n",
      "Iteration 8, loss = 0.00399921\n",
      "Iteration 9, loss = 0.00395240\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04459038\n",
      "Iteration 2, loss = 0.00686328\n",
      "Iteration 3, loss = 0.00427890\n",
      "Iteration 4, loss = 0.00407979\n",
      "Iteration 5, loss = 0.00401075\n",
      "Iteration 6, loss = 0.00396248\n",
      "Iteration 7, loss = 0.00390510\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01885059\n",
      "Iteration 2, loss = 0.00427831\n",
      "Iteration 3, loss = 0.00412012\n",
      "Iteration 4, loss = 0.00403850\n",
      "Iteration 5, loss = 0.00394307\n",
      "Iteration 6, loss = 0.00384575\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03585697\n",
      "Iteration 2, loss = 0.00416369\n",
      "Iteration 3, loss = 0.00398789\n",
      "Iteration 4, loss = 0.00394294\n",
      "Iteration 5, loss = 0.00389981\n",
      "Iteration 6, loss = 0.00388747\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00871458\n",
      "Iteration 2, loss = 0.00398674\n",
      "Iteration 3, loss = 0.00394431\n",
      "Iteration 4, loss = 0.00392878\n",
      "Iteration 5, loss = 0.00381649\n",
      "Iteration 6, loss = 0.00367608\n",
      "Iteration 7, loss = 0.00353766\n",
      "Iteration 8, loss = 0.00326968\n",
      "Iteration 9, loss = 0.00290574\n",
      "Iteration 10, loss = 0.00258670\n",
      "Iteration 11, loss = 0.00230840\n",
      "Iteration 12, loss = 0.00199866\n",
      "Iteration 13, loss = 0.00182643\n",
      "Iteration 14, loss = 0.00170455\n",
      "Iteration 15, loss = 0.00162047\n",
      "Iteration 16, loss = 0.00153378\n",
      "Iteration 17, loss = 0.00143669\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00600010\n",
      "Iteration 2, loss = 0.00414520\n",
      "Iteration 3, loss = 0.00406445\n",
      "Iteration 4, loss = 0.00410389\n",
      "Iteration 5, loss = 0.00391880\n",
      "Iteration 6, loss = 0.00390207\n",
      "Iteration 7, loss = 0.00379540\n",
      "Iteration 8, loss = 0.00366408\n",
      "Iteration 9, loss = 0.00354416\n",
      "Iteration 10, loss = 0.00325203\n",
      "Iteration 11, loss = 0.00294571\n",
      "Iteration 12, loss = 0.00259884\n",
      "Iteration 13, loss = 0.00221761\n",
      "Iteration 14, loss = 0.00201722\n",
      "Iteration 15, loss = 0.00173336\n",
      "Iteration 16, loss = 0.00165139\n",
      "Iteration 17, loss = 0.00155931\n",
      "Iteration 18, loss = 0.00148259\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00562958\n",
      "Iteration 2, loss = 0.00421302\n",
      "Iteration 3, loss = 0.00425845\n",
      "Iteration 4, loss = 0.00427042\n",
      "Iteration 5, loss = 0.00423422\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.08701872\n",
      "Iteration 2, loss = 0.01602201\n",
      "Iteration 3, loss = 0.00974285\n",
      "Iteration 4, loss = 0.00689905\n",
      "Iteration 5, loss = 0.00543010\n",
      "Iteration 6, loss = 0.00471661\n",
      "Iteration 7, loss = 0.00439476\n",
      "Iteration 8, loss = 0.00424874\n",
      "Iteration 9, loss = 0.00416995\n",
      "Iteration 10, loss = 0.00411827\n",
      "Iteration 11, loss = 0.00407129\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03053616\n",
      "Iteration 2, loss = 0.00569961\n",
      "Iteration 3, loss = 0.00416165\n",
      "Iteration 4, loss = 0.00407226\n",
      "Iteration 5, loss = 0.00403009\n",
      "Iteration 6, loss = 0.00399206\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03953660\n",
      "Iteration 2, loss = 0.00551193\n",
      "Iteration 3, loss = 0.00428200\n",
      "Iteration 4, loss = 0.00418447\n",
      "Iteration 5, loss = 0.00413762\n",
      "Iteration 6, loss = 0.00408075\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01752869\n",
      "Iteration 2, loss = 0.00422807\n",
      "Iteration 3, loss = 0.00417574\n",
      "Iteration 4, loss = 0.00409177\n",
      "Iteration 5, loss = 0.00398941\n",
      "Iteration 6, loss = 0.00386451\n",
      "Iteration 7, loss = 0.00374166\n",
      "Iteration 8, loss = 0.00361094\n",
      "Iteration 9, loss = 0.00341924\n",
      "Iteration 10, loss = 0.00325786\n",
      "Iteration 11, loss = 0.00301238\n",
      "Iteration 12, loss = 0.00273923\n",
      "Iteration 13, loss = 0.00251376\n",
      "Iteration 14, loss = 0.00226503\n",
      "Iteration 15, loss = 0.00208208\n",
      "Iteration 16, loss = 0.00191011\n",
      "Iteration 17, loss = 0.00178589\n",
      "Iteration 18, loss = 0.00170447\n",
      "Iteration 19, loss = 0.00165503\n",
      "Iteration 20, loss = 0.00156456\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01130309\n",
      "Iteration 2, loss = 0.00408627\n",
      "Iteration 3, loss = 0.00406039\n",
      "Iteration 4, loss = 0.00401601\n",
      "Iteration 5, loss = 0.00392739\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00565227\n",
      "Iteration 2, loss = 0.00420402\n",
      "Iteration 3, loss = 0.00416734\n",
      "Iteration 4, loss = 0.00406134\n",
      "Iteration 5, loss = 0.00410790\n",
      "Iteration 6, loss = 0.00386771\n",
      "Iteration 7, loss = 0.00385929\n",
      "Iteration 8, loss = 0.00368337\n",
      "Iteration 9, loss = 0.00341365\n",
      "Iteration 10, loss = 0.00319799\n",
      "Iteration 11, loss = 0.00299861\n",
      "Iteration 12, loss = 0.00259025\n",
      "Iteration 13, loss = 0.00230025\n",
      "Iteration 14, loss = 0.00204245\n",
      "Iteration 15, loss = 0.00191853\n",
      "Iteration 16, loss = 0.00173324\n",
      "Iteration 17, loss = 0.00164760\n",
      "Iteration 18, loss = 0.00156777\n",
      "Iteration 19, loss = 0.00154341\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00666833\n",
      "Iteration 2, loss = 0.00430144\n",
      "Iteration 3, loss = 0.00425806\n",
      "Iteration 4, loss = 0.00433892\n",
      "Iteration 5, loss = 0.00432399\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.09678182\n",
      "Iteration 2, loss = 0.02174184\n",
      "Iteration 3, loss = 0.00975295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.00598966\n",
      "Iteration 5, loss = 0.00483515\n",
      "Iteration 6, loss = 0.00448105\n",
      "Iteration 7, loss = 0.00436058\n",
      "Iteration 8, loss = 0.00430405\n",
      "Iteration 9, loss = 0.00426328\n",
      "Iteration 10, loss = 0.00422640\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03489142\n",
      "Iteration 2, loss = 0.00544103\n",
      "Iteration 3, loss = 0.00432264\n",
      "Iteration 4, loss = 0.00422986\n",
      "Iteration 5, loss = 0.00416284\n",
      "Iteration 6, loss = 0.00409455\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01866899\n",
      "Iteration 2, loss = 0.00426333\n",
      "Iteration 3, loss = 0.00411234\n",
      "Iteration 4, loss = 0.00401410\n",
      "Iteration 5, loss = 0.00392373\n",
      "Iteration 6, loss = 0.00384141\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02326044\n",
      "Iteration 2, loss = 0.00405469\n",
      "Iteration 3, loss = 0.00400397\n",
      "Iteration 4, loss = 0.00396023\n",
      "Iteration 5, loss = 0.00388629\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01909793\n",
      "Iteration 2, loss = 0.00395736\n",
      "Iteration 3, loss = 0.00391493\n",
      "Iteration 4, loss = 0.00392228\n",
      "Iteration 5, loss = 0.00389496\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00669574\n",
      "Iteration 2, loss = 0.00407433\n",
      "Iteration 3, loss = 0.00406969\n",
      "Iteration 4, loss = 0.00401014\n",
      "Iteration 5, loss = 0.00404581\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00503738\n",
      "Iteration 2, loss = 0.00429007\n",
      "Iteration 3, loss = 0.00424720\n",
      "Iteration 4, loss = 0.00419925\n",
      "Iteration 5, loss = 0.00421977\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05430113\n",
      "Iteration 2, loss = 0.01264842\n",
      "Iteration 3, loss = 0.00544528\n",
      "Iteration 4, loss = 0.00424015\n",
      "Iteration 5, loss = 0.00405079\n",
      "Iteration 6, loss = 0.00402305\n",
      "Iteration 7, loss = 0.00400653\n",
      "Iteration 8, loss = 0.00399437\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03717977\n",
      "Iteration 2, loss = 0.00680828\n",
      "Iteration 3, loss = 0.00430643\n",
      "Iteration 4, loss = 0.00406635\n",
      "Iteration 5, loss = 0.00402555\n",
      "Iteration 6, loss = 0.00398724\n",
      "Iteration 7, loss = 0.00395914\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01855866\n",
      "Iteration 2, loss = 0.00411808\n",
      "Iteration 3, loss = 0.00405452\n",
      "Iteration 4, loss = 0.00400544\n",
      "Iteration 5, loss = 0.00394457\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02212609\n",
      "Iteration 2, loss = 0.00416159\n",
      "Iteration 3, loss = 0.00411683\n",
      "Iteration 4, loss = 0.00408670\n",
      "Iteration 5, loss = 0.00402730\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00723789\n",
      "Iteration 2, loss = 0.00407013\n",
      "Iteration 3, loss = 0.00399231\n",
      "Iteration 4, loss = 0.00389257\n",
      "Iteration 5, loss = 0.00380150\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00607587\n",
      "Iteration 2, loss = 0.00415249\n",
      "Iteration 3, loss = 0.00412618\n",
      "Iteration 4, loss = 0.00410515\n",
      "Iteration 5, loss = 0.00408044\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00577708\n",
      "Iteration 2, loss = 0.00437417\n",
      "Iteration 3, loss = 0.00438505\n",
      "Iteration 4, loss = 0.00429168\n",
      "Iteration 5, loss = 0.00425362\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05082097\n",
      "Iteration 2, loss = 0.00984217\n",
      "Iteration 3, loss = 0.00475695\n",
      "Iteration 4, loss = 0.00411758\n",
      "Iteration 5, loss = 0.00403041\n",
      "Iteration 6, loss = 0.00400908\n",
      "Iteration 7, loss = 0.00399642\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.08804427\n",
      "Iteration 2, loss = 0.01050165\n",
      "Iteration 3, loss = 0.00491603\n",
      "Iteration 4, loss = 0.00452544\n",
      "Iteration 5, loss = 0.00445209\n",
      "Iteration 6, loss = 0.00439848\n",
      "Iteration 7, loss = 0.00435418\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02146843\n",
      "Iteration 2, loss = 0.00440086\n",
      "Iteration 3, loss = 0.00409359\n",
      "Iteration 4, loss = 0.00405004\n",
      "Iteration 5, loss = 0.00402441\n",
      "Iteration 6, loss = 0.00397264\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01011640\n",
      "Iteration 2, loss = 0.00393977\n",
      "Iteration 3, loss = 0.00390731\n",
      "Iteration 4, loss = 0.00380357\n",
      "Iteration 5, loss = 0.00372910\n",
      "Iteration 6, loss = 0.00359101\n",
      "Iteration 7, loss = 0.00345553\n",
      "Iteration 8, loss = 0.00332420\n",
      "Iteration 9, loss = 0.00310906\n",
      "Iteration 10, loss = 0.00290217\n",
      "Iteration 11, loss = 0.00266254\n",
      "Iteration 12, loss = 0.00243474\n",
      "Iteration 13, loss = 0.00219224\n",
      "Iteration 14, loss = 0.00202183\n",
      "Iteration 15, loss = 0.00188021\n",
      "Iteration 16, loss = 0.00178471\n",
      "Iteration 17, loss = 0.00169262\n",
      "Iteration 18, loss = 0.00162628\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00817529\n",
      "Iteration 2, loss = 0.00403745\n",
      "Iteration 3, loss = 0.00399306\n",
      "Iteration 4, loss = 0.00391607\n",
      "Iteration 5, loss = 0.00375002\n",
      "Iteration 6, loss = 0.00360993\n",
      "Iteration 7, loss = 0.00343894\n",
      "Iteration 8, loss = 0.00331587\n",
      "Iteration 9, loss = 0.00302909\n",
      "Iteration 10, loss = 0.00279668\n",
      "Iteration 11, loss = 0.00257488\n",
      "Iteration 12, loss = 0.00234833\n",
      "Iteration 13, loss = 0.00214923\n",
      "Iteration 14, loss = 0.00199810\n",
      "Iteration 15, loss = 0.00187395\n",
      "Iteration 16, loss = 0.00172392\n",
      "Iteration 17, loss = 0.00160936\n",
      "Iteration 18, loss = 0.00152290\n",
      "Iteration 19, loss = 0.00147841\n",
      "Iteration 20, loss = 0.00142041\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00576960\n",
      "Iteration 2, loss = 0.00404152\n",
      "Iteration 3, loss = 0.00405352\n",
      "Iteration 4, loss = 0.00397150\n",
      "Iteration 5, loss = 0.00386800\n",
      "Iteration 6, loss = 0.00387519\n",
      "Iteration 7, loss = 0.00384166\n",
      "Iteration 8, loss = 0.00352595\n",
      "Iteration 9, loss = 0.00331127\n",
      "Iteration 10, loss = 0.00316218\n",
      "Iteration 11, loss = 0.00288730\n",
      "Iteration 12, loss = 0.00259700\n",
      "Iteration 13, loss = 0.00222436\n",
      "Iteration 14, loss = 0.00200180\n",
      "Iteration 15, loss = 0.00185371\n",
      "Iteration 16, loss = 0.00171805\n",
      "Iteration 17, loss = 0.00164435\n",
      "Iteration 18, loss = 0.00151471\n",
      "Iteration 19, loss = 0.00147084\n",
      "Iteration 20, loss = 0.00143703\n",
      "Iteration 21, loss = 0.00136827\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00534126\n",
      "Iteration 2, loss = 0.00413264\n",
      "Iteration 3, loss = 0.00420606\n",
      "Iteration 4, loss = 0.00431744\n",
      "Iteration 5, loss = 0.00415013\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.09860999\n",
      "Iteration 2, loss = 0.01561830\n",
      "Iteration 3, loss = 0.00906528\n",
      "Iteration 4, loss = 0.00650149\n",
      "Iteration 5, loss = 0.00525475\n",
      "Iteration 6, loss = 0.00464521\n",
      "Iteration 7, loss = 0.00434323\n",
      "Iteration 8, loss = 0.00419334\n",
      "Iteration 9, loss = 0.00411060\n",
      "Iteration 10, loss = 0.00405466\n",
      "Iteration 11, loss = 0.00401213\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06479654\n",
      "Iteration 2, loss = 0.00768953\n",
      "Iteration 3, loss = 0.00463473\n",
      "Iteration 4, loss = 0.00432981\n",
      "Iteration 5, loss = 0.00425445\n",
      "Iteration 6, loss = 0.00422030\n",
      "Iteration 7, loss = 0.00419917\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02136493\n",
      "Iteration 2, loss = 0.00436782\n",
      "Iteration 3, loss = 0.00419932\n",
      "Iteration 4, loss = 0.00411900\n",
      "Iteration 5, loss = 0.00402225\n",
      "Iteration 6, loss = 0.00395486\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02101638\n",
      "Iteration 2, loss = 0.00419934\n",
      "Iteration 3, loss = 0.00417610\n",
      "Iteration 4, loss = 0.00414812\n",
      "Iteration 5, loss = 0.00408272\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01235767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.00405908\n",
      "Iteration 3, loss = 0.00400940\n",
      "Iteration 4, loss = 0.00393742\n",
      "Iteration 5, loss = 0.00388535\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00649683\n",
      "Iteration 2, loss = 0.00422843\n",
      "Iteration 3, loss = 0.00418470\n",
      "Iteration 4, loss = 0.00412563\n",
      "Iteration 5, loss = 0.00408703\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00522593\n",
      "Iteration 2, loss = 0.00434597\n",
      "Iteration 3, loss = 0.00432211\n",
      "Iteration 4, loss = 0.00426616\n",
      "Iteration 5, loss = 0.00426866\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.07261536\n",
      "Iteration 2, loss = 0.01458847\n",
      "Iteration 3, loss = 0.00578249\n",
      "Iteration 4, loss = 0.00440059\n",
      "Iteration 5, loss = 0.00418476\n",
      "Iteration 6, loss = 0.00410947\n",
      "Iteration 7, loss = 0.00404946\n",
      "Iteration 8, loss = 0.00399451\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03196468\n",
      "Iteration 2, loss = 0.00688469\n",
      "Iteration 3, loss = 0.00454901\n",
      "Iteration 4, loss = 0.00416117\n",
      "Iteration 5, loss = 0.00405080\n",
      "Iteration 6, loss = 0.00399327\n",
      "Iteration 7, loss = 0.00396266\n",
      "Iteration 8, loss = 0.00389617\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01418594\n",
      "Iteration 2, loss = 0.00429959\n",
      "Iteration 3, loss = 0.00417256\n",
      "Iteration 4, loss = 0.00408304\n",
      "Iteration 5, loss = 0.00399702\n",
      "Iteration 6, loss = 0.00391744\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00862510\n",
      "Iteration 2, loss = 0.00405885\n",
      "Iteration 3, loss = 0.00393790\n",
      "Iteration 4, loss = 0.00388655\n",
      "Iteration 5, loss = 0.00373053\n",
      "Iteration 6, loss = 0.00366527\n",
      "Iteration 7, loss = 0.00353710\n",
      "Iteration 8, loss = 0.00334796\n",
      "Iteration 9, loss = 0.00319621\n",
      "Iteration 10, loss = 0.00292882\n",
      "Iteration 11, loss = 0.00267875\n",
      "Iteration 12, loss = 0.00247890\n",
      "Iteration 13, loss = 0.00225602\n",
      "Iteration 14, loss = 0.00205968\n",
      "Iteration 15, loss = 0.00192852\n",
      "Iteration 16, loss = 0.00179116\n",
      "Iteration 17, loss = 0.00170006\n",
      "Iteration 18, loss = 0.00163021\n",
      "Iteration 19, loss = 0.00153840\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00859970\n",
      "Iteration 2, loss = 0.00406679\n",
      "Iteration 3, loss = 0.00391755\n",
      "Iteration 4, loss = 0.00387580\n",
      "Iteration 5, loss = 0.00378555\n",
      "Iteration 6, loss = 0.00367892\n",
      "Iteration 7, loss = 0.00354518\n",
      "Iteration 8, loss = 0.00338230\n",
      "Iteration 9, loss = 0.00311070\n",
      "Iteration 10, loss = 0.00282349\n",
      "Iteration 11, loss = 0.00250020\n",
      "Iteration 12, loss = 0.00219869\n",
      "Iteration 13, loss = 0.00200286\n",
      "Iteration 14, loss = 0.00183979\n",
      "Iteration 15, loss = 0.00169697\n",
      "Iteration 16, loss = 0.00159113\n",
      "Iteration 17, loss = 0.00151367\n",
      "Iteration 18, loss = 0.00141692\n",
      "Iteration 19, loss = 0.00137086\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00817860\n",
      "Iteration 2, loss = 0.00400588\n",
      "Iteration 3, loss = 0.00399875\n",
      "Iteration 4, loss = 0.00397248\n",
      "Iteration 5, loss = 0.00398198\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00905331\n",
      "Iteration 2, loss = 0.00406200\n",
      "Iteration 3, loss = 0.00409385\n",
      "Iteration 4, loss = 0.00401062\n",
      "Iteration 5, loss = 0.00408358\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04704773\n",
      "Iteration 2, loss = 0.01115096\n",
      "Iteration 3, loss = 0.00520489\n",
      "Iteration 4, loss = 0.00426037\n",
      "Iteration 5, loss = 0.00411234\n",
      "Iteration 6, loss = 0.00407842\n",
      "Iteration 7, loss = 0.00405216\n",
      "Iteration 8, loss = 0.00403407\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.09277156\n",
      "Iteration 2, loss = 0.01125071\n",
      "Iteration 3, loss = 0.00552165\n",
      "Iteration 4, loss = 0.00493769\n",
      "Iteration 5, loss = 0.00479530\n",
      "Iteration 6, loss = 0.00470099\n",
      "Iteration 7, loss = 0.00460179\n",
      "Iteration 8, loss = 0.00450925\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05754249\n",
      "Iteration 2, loss = 0.00501514\n",
      "Iteration 3, loss = 0.00466325\n",
      "Iteration 4, loss = 0.00459867\n",
      "Iteration 5, loss = 0.00453071\n",
      "Iteration 6, loss = 0.00446834\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01993492\n",
      "Iteration 2, loss = 0.00400027\n",
      "Iteration 3, loss = 0.00396370\n",
      "Iteration 4, loss = 0.00388703\n",
      "Iteration 5, loss = 0.00380998\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01434687\n",
      "Iteration 2, loss = 0.00408551\n",
      "Iteration 3, loss = 0.00406573\n",
      "Iteration 4, loss = 0.00404826\n",
      "Iteration 5, loss = 0.00399789\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00662213\n",
      "Iteration 2, loss = 0.00410384\n",
      "Iteration 3, loss = 0.00413110\n",
      "Iteration 4, loss = 0.00404329\n",
      "Iteration 5, loss = 0.00395121\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00627586\n",
      "Iteration 2, loss = 0.00429938\n",
      "Iteration 3, loss = 0.00431630\n",
      "Iteration 4, loss = 0.00429554\n",
      "Iteration 5, loss = 0.00427385\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.40412084\n",
      "Iteration 2, loss = 0.07974033\n",
      "Iteration 3, loss = 0.03471659\n",
      "Iteration 4, loss = 0.02064035\n",
      "Iteration 5, loss = 0.01308540\n",
      "Iteration 6, loss = 0.00894171\n",
      "Iteration 7, loss = 0.00671342\n",
      "Iteration 8, loss = 0.00555340\n",
      "Iteration 9, loss = 0.00497359\n",
      "Iteration 10, loss = 0.00469181\n",
      "Iteration 11, loss = 0.00455212\n",
      "Iteration 12, loss = 0.00447512\n",
      "Iteration 13, loss = 0.00443017\n",
      "Iteration 14, loss = 0.00439839\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.10177253\n",
      "Iteration 2, loss = 0.01655933\n",
      "Iteration 3, loss = 0.00715339\n",
      "Iteration 4, loss = 0.00501031\n",
      "Iteration 5, loss = 0.00457087\n",
      "Iteration 6, loss = 0.00446249\n",
      "Iteration 7, loss = 0.00441569\n",
      "Iteration 8, loss = 0.00437053\n",
      "Iteration 9, loss = 0.00432237\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.02802275\n",
      "Iteration 2, loss = 0.00473951\n",
      "Iteration 3, loss = 0.00425699\n",
      "Iteration 4, loss = 0.00418764\n",
      "Iteration 5, loss = 0.00413600\n",
      "Iteration 6, loss = 0.00405917\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01343210\n",
      "Iteration 2, loss = 0.00394066\n",
      "Iteration 3, loss = 0.00387458\n",
      "Iteration 4, loss = 0.00381103\n",
      "Iteration 5, loss = 0.00371223\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01334479\n",
      "Iteration 2, loss = 0.00402129\n",
      "Iteration 3, loss = 0.00398476\n",
      "Iteration 4, loss = 0.00392796\n",
      "Iteration 5, loss = 0.00389792\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00673327\n",
      "Iteration 2, loss = 0.00406067\n",
      "Iteration 3, loss = 0.00400956\n",
      "Iteration 4, loss = 0.00397830\n",
      "Iteration 5, loss = 0.00403321\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00505348\n",
      "Iteration 2, loss = 0.00432619\n",
      "Iteration 3, loss = 0.00426000\n",
      "Iteration 4, loss = 0.00417741\n",
      "Iteration 5, loss = 0.00415599\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.25917533\n",
      "Iteration 2, loss = 0.04480894\n",
      "Iteration 3, loss = 0.01759150\n",
      "Iteration 4, loss = 0.00893779\n",
      "Iteration 5, loss = 0.00604486\n",
      "Iteration 6, loss = 0.00505514\n",
      "Iteration 7, loss = 0.00470009\n",
      "Iteration 8, loss = 0.00455370\n",
      "Iteration 9, loss = 0.00447851\n",
      "Iteration 10, loss = 0.00442671\n",
      "Iteration 11, loss = 0.00438455\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.03094928\n",
      "Iteration 2, loss = 0.00620236\n",
      "Iteration 3, loss = 0.00441708\n",
      "Iteration 4, loss = 0.00416979\n",
      "Iteration 5, loss = 0.00413085\n",
      "Iteration 6, loss = 0.00408187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.00402232\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.04529716\n",
      "Iteration 2, loss = 0.00571518\n",
      "Iteration 3, loss = 0.00451018\n",
      "Iteration 4, loss = 0.00441547\n",
      "Iteration 5, loss = 0.00436638\n",
      "Iteration 6, loss = 0.00431522\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01653716\n",
      "Iteration 2, loss = 0.00422140\n",
      "Iteration 3, loss = 0.00416538\n",
      "Iteration 4, loss = 0.00407676\n",
      "Iteration 5, loss = 0.00399117\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.01132788\n",
      "Iteration 2, loss = 0.00411438\n",
      "Iteration 3, loss = 0.00407631\n",
      "Iteration 4, loss = 0.00401873\n",
      "Iteration 5, loss = 0.00395066\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00819710\n",
      "Iteration 2, loss = 0.00411034\n",
      "Iteration 3, loss = 0.00410131\n",
      "Iteration 4, loss = 0.00408535\n",
      "Iteration 5, loss = 0.00401737\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00550049\n",
      "Iteration 2, loss = 0.00453134\n",
      "Iteration 3, loss = 0.00435260\n",
      "Iteration 4, loss = 0.00432712\n",
      "Iteration 5, loss = 0.00428705\n",
      "Iteration 6, loss = 0.00426632\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njDHGhGXJwhhjTFj/D0Jcp4HOMjTRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11977c410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "enc = OneHotEncoder()\n",
    "feature_data = Data.iloc[:,:5]\n",
    "feature_data_OneHotEncode = enc.fit_transform(feature_data).toarray()\n",
    "target_data = Data[labels[5]]\n",
    "target_data_array = np.asarray(target_data)\n",
    "\n",
    "\n",
    "num_neurons = [5,10,20,50,100,200,400]\n",
    "activation_function = ['relu', 'logistic', 'tanh']\n",
    "\n",
    "for af in activation_function:\n",
    "    kf = KFold(n_splits=10)\n",
    "    train_mse = [0] * len(num_neurons)\n",
    "    test_mse = [0] * len(num_neurons)\n",
    "    for train_index, test_index in kf.split(feature_data_OneHotEncode):\n",
    "        train_sample = feature_data_OneHotEncode[train_index, :]\n",
    "        train_target = target_data_array[train_index]\n",
    "\n",
    "        test_sample = feature_data_OneHotEncode[test_index, :]\n",
    "        test_target = target_data_array[test_index]\n",
    "\n",
    "        for i, n in enumerate(num_neurons):\n",
    "            clf = MLPRegressor(activation= af, alpha=1e-5, hidden_layer_sizes=(n,), verbose = 'True', learning_rate = 'adaptive')\n",
    "            clf.fit(train_sample, train_target)\n",
    "           \n",
    "            train_predicted = clf.predict(train_sample)\n",
    "            test_predicted = clf.predict(test_sample)\n",
    "                \n",
    "            train_mse[i] += mean_squared_error(train_target, train_predicted)\n",
    "            test_mse[i] += mean_squared_error(test_target, test_predicted)\n",
    "    train_rmse = map(lambda x: (x / 10) ** 0.5, train_mse)\n",
    "    test_rmse = map(lambda x: (x / 10) ** 0.5, test_mse)\n",
    "\n",
    "\n",
    "    plt.plot(num_neurons, test_rmse, label = af)\n",
    "    #plt.plot(num_neurons, train_rmse, label = \"train_rmse\")\n",
    "    plt.xlabel('number of neurons')\n",
    "    plt.ylabel('Test RMSE')\n",
    "    plt.title('Test RMSE on Diff AFs with Various Neurons' )\n",
    "    plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(target, predict, title = \"\"):\n",
    "    x = range(len(predict))\n",
    "    \n",
    "    area = np.pi * (1)**2  # 0 to 15 point radii\n",
    "    plt.scatter(x, predict, color = 'red', s=area, label='prediction')\n",
    "    plt.scatter(x, target, color = 'blue', s=area, label='True Value')\n",
    "    plt.title(\"fitted values against true values\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(x, predict, color = 'red',s=area, label='prediction')\n",
    "    plt.scatter(x, target - predict, color = 'blue', s=area, label='Residuals')\n",
    "    plt.title(\"residuals versus fitted values\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.00492075\n",
      "Iteration 2, loss = 0.00216032\n",
      "Iteration 3, loss = 0.00158643\n",
      "Iteration 4, loss = 0.00122888\n",
      "Iteration 5, loss = 0.00091889\n",
      "Iteration 6, loss = 0.00066190\n",
      "Iteration 7, loss = 0.00050623\n",
      "Iteration 8, loss = 0.00043156\n",
      "Iteration 9, loss = 0.00039084\n",
      "Iteration 10, loss = 0.00036031\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00669651\n",
      "Iteration 2, loss = 0.00267414\n",
      "Iteration 3, loss = 0.00191685\n",
      "Iteration 4, loss = 0.00149768\n",
      "Iteration 5, loss = 0.00116219\n",
      "Iteration 6, loss = 0.00085081\n",
      "Iteration 7, loss = 0.00062833\n",
      "Iteration 8, loss = 0.00050695\n",
      "Iteration 9, loss = 0.00044319\n",
      "Iteration 10, loss = 0.00040904\n",
      "Iteration 11, loss = 0.00037925\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00747627\n",
      "Iteration 2, loss = 0.00270142\n",
      "Iteration 3, loss = 0.00188969\n",
      "Iteration 4, loss = 0.00147977\n",
      "Iteration 5, loss = 0.00114341\n",
      "Iteration 6, loss = 0.00085948\n",
      "Iteration 7, loss = 0.00064695\n",
      "Iteration 8, loss = 0.00053198\n",
      "Iteration 9, loss = 0.00046694\n",
      "Iteration 10, loss = 0.00042653\n",
      "Iteration 11, loss = 0.00039752\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00570840\n",
      "Iteration 2, loss = 0.00236642\n",
      "Iteration 3, loss = 0.00174101\n",
      "Iteration 4, loss = 0.00138426\n",
      "Iteration 5, loss = 0.00104321\n",
      "Iteration 6, loss = 0.00075020\n",
      "Iteration 7, loss = 0.00054303\n",
      "Iteration 8, loss = 0.00044749\n",
      "Iteration 9, loss = 0.00040194\n",
      "Iteration 10, loss = 0.00037098\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00478562\n",
      "Iteration 2, loss = 0.00209864\n",
      "Iteration 3, loss = 0.00151824\n",
      "Iteration 4, loss = 0.00112316\n",
      "Iteration 5, loss = 0.00081724\n",
      "Iteration 6, loss = 0.00059080\n",
      "Iteration 7, loss = 0.00047217\n",
      "Iteration 8, loss = 0.00041005\n",
      "Iteration 9, loss = 0.00038371\n",
      "Iteration 10, loss = 0.00035805\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00778738\n",
      "Iteration 2, loss = 0.00288930\n",
      "Iteration 3, loss = 0.00201436\n",
      "Iteration 4, loss = 0.00163139\n",
      "Iteration 5, loss = 0.00134793\n",
      "Iteration 6, loss = 0.00104667\n",
      "Iteration 7, loss = 0.00076853\n",
      "Iteration 8, loss = 0.00058826\n",
      "Iteration 9, loss = 0.00047923\n",
      "Iteration 10, loss = 0.00042600\n",
      "Iteration 11, loss = 0.00039270\n",
      "Iteration 12, loss = 0.00037125\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00582856\n",
      "Iteration 2, loss = 0.00234533\n",
      "Iteration 3, loss = 0.00171335\n",
      "Iteration 4, loss = 0.00133924\n",
      "Iteration 5, loss = 0.00103938\n",
      "Iteration 6, loss = 0.00079521\n",
      "Iteration 7, loss = 0.00061632\n",
      "Iteration 8, loss = 0.00050454\n",
      "Iteration 9, loss = 0.00044575\n",
      "Iteration 10, loss = 0.00041038\n",
      "Iteration 11, loss = 0.00038099\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00561660\n",
      "Iteration 2, loss = 0.00230350\n",
      "Iteration 3, loss = 0.00163936\n",
      "Iteration 4, loss = 0.00121257\n",
      "Iteration 5, loss = 0.00087493\n",
      "Iteration 6, loss = 0.00063617\n",
      "Iteration 7, loss = 0.00050117\n",
      "Iteration 8, loss = 0.00043462\n",
      "Iteration 9, loss = 0.00039414\n",
      "Iteration 10, loss = 0.00036718\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00736481\n",
      "Iteration 2, loss = 0.00285137\n",
      "Iteration 3, loss = 0.00199200\n",
      "Iteration 4, loss = 0.00160492\n",
      "Iteration 5, loss = 0.00130171\n",
      "Iteration 6, loss = 0.00104574\n",
      "Iteration 7, loss = 0.00080594\n",
      "Iteration 8, loss = 0.00062131\n",
      "Iteration 9, loss = 0.00050729\n",
      "Iteration 10, loss = 0.00044225\n",
      "Iteration 11, loss = 0.00040240\n",
      "Iteration 12, loss = 0.00037119\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.00677329\n",
      "Iteration 2, loss = 0.00271529\n",
      "Iteration 3, loss = 0.00195829\n",
      "Iteration 4, loss = 0.00158540\n",
      "Iteration 5, loss = 0.00123854\n",
      "Iteration 6, loss = 0.00093791\n",
      "Iteration 7, loss = 0.00070569\n",
      "Iteration 8, loss = 0.00055100\n",
      "Iteration 9, loss = 0.00046411\n",
      "Iteration 10, loss = 0.00041888\n",
      "Iteration 11, loss = 0.00038844\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "clf = MLPRegressor(alpha = 0.001, hidden_layer_sizes = (200, ), \\\n",
    "                           activation = 'relu', verbose = 'True', learning_rate = 'adaptive')\n",
    "\n",
    "predicted = cross_val_predict(clf, feature_data_OneHotEncode, target_data_array, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119643750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEICAYAAAC3Y/QeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIAhCG/L/AR6g3rz/tyypAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11950da10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result(target_data_array, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
